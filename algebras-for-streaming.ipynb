{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// put the spark install from the base notebook image onto Ammonite's classpath\n",
    "java.nio.file.Files.list(java.nio.file.Paths.get(\"/opt/spark/jars\")).toArray.map(_.toString).foreach { fname =>\n",
    "  val path = java.nio.file.FileSystems.getDefault().getPath(fname)\n",
    "  val x = ammonite.ops.Path(path)\n",
    "  interp.load.cp(x)\n",
    "}\n",
    "// Load the ammonite-spark package to get AmmoniteSparkSession\n",
    "import $ivy.`sh.almond::ammonite-spark:0.1.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql-kafka-0-10:2.2.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://repo1.maven.org/maven2/org/isarnproject/isarn-sketches-spark_2.11/0.3.1-topk-1-sp2.2-py2.7/isarn-sketches-spark_2.11-0.3.1-topk-1-sp2.2-py2.7.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/isarnproject/isarn-sketches-spark_2.11/0.3.1-topk-1-sp2.2-py2.7/isarn-sketches-spark_2.11-0.3.1-topk-1-sp2.2-py2.7.pom.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/org/isarnproject/isarn-sketches-spark_2.11/0.3.1-topk-1-sp2.2-py2.7/isarn-sketches-spark_2.11-0.3.1-topk-1-sp2.2-py2.7.pom.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/org/isarnproject/isarn-sketches-spark_2.11/0.3.1-topk-1-sp2.2-py2.7/isarn-sketches-spark_2.11-0.3.1-topk-1-sp2.2-py2.7.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/isarnproject/isarn-sketches-spark_2.11/0.3.1-topk-1-sp2.2-py2.7/\n",
      "Downloaded https://repo1.maven.org/maven2/org/isarnproject/isarn-sketches-spark_2.11/0.3.1-topk-1-sp2.2-py2.7/\n",
      "Downloading https://repo1.maven.org/maven2/org/isarnproject/isarn-sketches-spark_2.11/0.3.1-topk-1-sp2.2-py2.7/isarn-sketches-spark_2.11-0.3.1-topk-1-sp2.2-py2.7.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/org/isarnproject/isarn-sketches-spark_2.11/0.3.1-topk-1-sp2.2-py2.7/isarn-sketches-spark_2.11-0.3.1-topk-1-sp2.2-py2.7.jar\n",
      "Downloaded https://repo1.maven.org/maven2/org/isarnproject/isarn-sketches-spark_2.11/0.3.1-topk-1-sp2.2-py2.7/isarn-sketches-spark_2.11-0.3.1-topk-1-sp2.2-py2.7.jar\n",
      "Downloaded https://repo1.maven.org/maven2/org/isarnproject/isarn-sketches-spark_2.11/0.3.1-topk-1-sp2.2-py2.7/isarn-sketches-spark_2.11-0.3.1-topk-1-sp2.2-py2.7.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/org/isarnproject/isarn-sketches-spark_2.11/0.3.1-topk-1-sp2.2-py2.7/isarn-sketches-spark_2.11-0.3.1-topk-1-sp2.2-py2.7-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/org/isarnproject/isarn-sketches-spark_2.11/0.3.1-topk-1-sp2.2-py2.7/isarn-sketches-spark_2.11-0.3.1-topk-1-sp2.2-py2.7-sources.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/org/isarnproject/isarn-sketches-spark_2.11/0.3.1-topk-1-sp2.2-py2.7/isarn-sketches-spark_2.11-0.3.1-topk-1-sp2.2-py2.7-sources.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/org/isarnproject/isarn-sketches-spark_2.11/0.3.1-topk-1-sp2.2-py2.7/isarn-sketches-spark_2.11-0.3.1-topk-1-sp2.2-py2.7-sources.jar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                \u001b[39m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//\"org.isarnproject\" %% \"isarn-sketches-spark\" % \"0.3.1-sp2.2-py2.7\"\n",
    "//import $ivy.`org.isarnproject::isarn-sketches-spark:0.3.1-sp2.2-py2.7`\n",
    "import $ivy.`org.isarnproject::isarn-sketches-spark:0.3.1-topk-1-sp2.2-py2.7`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting spark JARs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.eclipse.jetty.util.log).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "18/09/24 23:11:46 INFO SparkContext: Running Spark version 2.2.0\n",
      "18/09/24 23:11:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "18/09/24 23:11:47 INFO SparkContext: Submitted application: cee2c3a0-36f5-4799-a721-a3d69d5d2586\n",
      "18/09/24 23:11:47 INFO SecurityManager: Changing view acls to: 1000130000\n",
      "18/09/24 23:11:47 INFO SecurityManager: Changing modify acls to: 1000130000\n",
      "18/09/24 23:11:47 INFO SecurityManager: Changing view acls groups to: \n",
      "18/09/24 23:11:47 INFO SecurityManager: Changing modify acls groups to: \n",
      "18/09/24 23:11:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1000130000); groups with view permissions: Set(); users  with modify permissions: Set(1000130000); groups with modify permissions: Set()\n",
      "18/09/24 23:11:47 INFO Utils: Successfully started service 'sparkDriver' on port 37187.\n",
      "18/09/24 23:11:47 INFO SparkEnv: Registering MapOutputTracker\n",
      "18/09/24 23:11:47 INFO SparkEnv: Registering BlockManagerMaster\n",
      "18/09/24 23:11:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "18/09/24 23:11:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "18/09/24 23:11:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cd1edcee-afb4-41bd-b05f-dcae35fe8bd0\n",
      "18/09/24 23:11:47 INFO MemoryStore: MemoryStore started with capacity 1909.8 MB\n",
      "18/09/24 23:11:47 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "18/09/24 23:11:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "18/09/24 23:11:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.17.0.13:4040\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/upickle_2.11/0.6.6/upickle_2.11-0.6.6.jar at spark://172.17.0.13:37187/jars/upickle_2.11-0.6.6.jar with timestamp 1537830708147\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fastparse_2.11/1.0.0/fastparse_2.11-1.0.0.jar at spark://172.17.0.13:37187/jars/fastparse_2.11-1.0.0.jar with timestamp 1537830708148\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/jline/jline-terminal/3.6.2/jline-terminal-3.6.2.jar at spark://172.17.0.13:37187/jars/jline-terminal-3.6.2.jar with timestamp 1537830708148\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/scala-kernel-api_2.11.12/0.1.7/scala-kernel-api_2.11.12-0.1.7.jar at spark://172.17.0.13:37187/jars/scala-kernel-api_2.11.12-0.1.7.jar with timestamp 1537830708148\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/io/get-coursier/coursier_2.11/1.1.0-M7/coursier_2.11-1.1.0-M7.jar at spark://172.17.0.13:37187/jars/coursier_2.11-1.1.0-M7.jar with timestamp 1537830708149\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar at spark://172.17.0.13:37187/jars/javassist-3.21.0-GA.jar with timestamp 1537830708149\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scalaj/scalaj-http_2.11/2.4.0/scalaj-http_2.11-2.4.0.jar at spark://172.17.0.13:37187/jars/scalaj-http_2.11-2.4.0.jar with timestamp 1537830708149\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fansi_2.11/0.2.4/fansi_2.11-0.2.4.jar at spark://172.17.0.13:37187/jars/fansi_2.11-0.2.4.jar with timestamp 1537830708149\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.11/0.1.4/sourcecode_2.11-0.1.4.jar at spark://172.17.0.13:37187/jars/sourcecode_2.11-0.1.4.jar with timestamp 1537830708149\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/github/scopt/scopt_2.11/3.5.0/scopt_2.11-3.5.0.jar at spark://172.17.0.13:37187/jars/scopt_2.11-3.5.0.jar with timestamp 1537830708149\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/jline/jline-reader/3.6.2/jline-reader-3.6.2.jar at spark://172.17.0.13:37187/jars/jline-reader-3.6.2.jar with timestamp 1537830708149\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/net/java/dev/jna/jna/4.2.2/jna-4.2.2.jar at spark://172.17.0.13:37187/jars/jna-4.2.2.jar with timestamp 1537830708150\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-interp_2.11.12/1.1.2-37-53fcfcd/ammonite-interp_2.11.12-1.1.2-37-53fcfcd.jar at spark://172.17.0.13:37187/jars/ammonite-interp_2.11.12-1.1.2-37-53fcfcd.jar with timestamp 1537830708150\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-repl_2.11.12/1.1.2-37-53fcfcd/ammonite-repl_2.11.12-1.1.2-37-53fcfcd.jar at spark://172.17.0.13:37187/jars/ammonite-repl_2.11.12-1.1.2-37-53fcfcd.jar with timestamp 1537830708150\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-util_2.11/1.1.2-37-53fcfcd/ammonite-util_2.11-1.1.2-37-53fcfcd.jar at spark://172.17.0.13:37187/jars/ammonite-util_2.11-1.1.2-37-53fcfcd.jar with timestamp 1537830708150\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/pprint_2.11/0.5.2/pprint_2.11-0.5.2.jar at spark://172.17.0.13:37187/jars/pprint_2.11-0.5.2.jar with timestamp 1537830708151\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-ops_2.11/1.1.2-37-53fcfcd/ammonite-ops_2.11-1.1.2-37-53fcfcd.jar at spark://172.17.0.13:37187/jars/ammonite-ops_2.11-1.1.2-37-53fcfcd.jar with timestamp 1537830708151\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/io/get-coursier/coursier-cache_2.11/1.1.0-M7/coursier-cache_2.11-1.1.0-M7.jar at spark://172.17.0.13:37187/jars/coursier-cache_2.11-1.1.0-M7.jar with timestamp 1537830708151\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-runtime_2.11/1.1.2-37-53fcfcd/ammonite-runtime_2.11-1.1.2-37-53fcfcd.jar at spark://172.17.0.13:37187/jars/ammonite-runtime_2.11-1.1.2-37-53fcfcd.jar with timestamp 1537830708152\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/geny_2.11/0.1.2/geny_2.11-0.1.2.jar at spark://172.17.0.13:37187/jars/geny_2.11-0.1.2.jar with timestamp 1537830708153\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/scalaparse_2.11/1.0.0/scalaparse_2.11-1.0.0.jar at spark://172.17.0.13:37187/jars/scalaparse_2.11-1.0.0.jar with timestamp 1537830708153\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-terminal_2.11/1.1.2-37-53fcfcd/ammonite-terminal_2.11-1.1.2-37-53fcfcd.jar at spark://172.17.0.13:37187/jars/ammonite-terminal_2.11-1.1.2-37-53fcfcd.jar with timestamp 1537830708155\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/interpreter-api_2.11/0.1.7/interpreter-api_2.11-0.1.7.jar at spark://172.17.0.13:37187/jars/interpreter-api_2.11-0.1.7.jar with timestamp 1537830708156\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fastparse-utils_2.11/1.0.0/fastparse-utils_2.11-1.0.0.jar at spark://172.17.0.13:37187/jars/fastparse-utils_2.11-1.0.0.jar with timestamp 1537830708157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.1.0/scala-xml_2.11-1.1.0.jar at spark://172.17.0.13:37187/jars/scala-xml_2.11-1.1.0.jar with timestamp 1537830708157\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/jline/jline-terminal-jna/3.6.2/jline-terminal-jna-3.6.2.jar at spark://172.17.0.13:37187/jars/jline-terminal-jna-3.6.2.jar with timestamp 1537830708158\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ujson_2.11/0.6.6/ujson_2.11-0.6.6.jar at spark://172.17.0.13:37187/jars/ujson_2.11-0.6.6.jar with timestamp 1537830708159\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/github/javaparser/javaparser-core/3.2.5/javaparser-core-3.2.5.jar at spark://172.17.0.13:37187/jars/javaparser-core-3.2.5.jar with timestamp 1537830708159\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/home/nbuser/.local/share/jupyter/kernels/scala/launcher.jar at spark://172.17.0.13:37187/jars/launcher.jar with timestamp 1537830708160\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-continuation/8.1.14.v20131031/jetty-continuation-8.1.14.v20131031.jar at spark://172.17.0.13:37187/jars/jetty-continuation-8.1.14.v20131031.jar with timestamp 1537830708161\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-http/8.1.14.v20131031/jetty-http-8.1.14.v20131031.jar at spark://172.17.0.13:37187/jars/jetty-http-8.1.14.v20131031.jar with timestamp 1537830708162\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-server/8.1.14.v20131031/jetty-server-8.1.14.v20131031.jar at spark://172.17.0.13:37187/jars/jetty-server-8.1.14.v20131031.jar with timestamp 1537830708163\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/ammonite-spark_2.11/0.1.1/ammonite-spark_2.11-0.1.1.jar at spark://172.17.0.13:37187/jars/ammonite-spark_2.11-0.1.1.jar with timestamp 1537830708163\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-io/8.1.14.v20131031/jetty-io-8.1.14.v20131031.jar at spark://172.17.0.13:37187/jars/jetty-io-8.1.14.v20131031.jar with timestamp 1537830708164\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/8.1.14.v20131031/jetty-util-8.1.14.v20131031.jar at spark://172.17.0.13:37187/jars/jetty-util-8.1.14.v20131031.jar with timestamp 1537830708164\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/JavaEWAH-0.3.2.jar at spark://172.17.0.13:37187/jars/JavaEWAH-0.3.2.jar with timestamp 1537830708164\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/RoaringBitmap-0.5.11.jar at spark://172.17.0.13:37187/jars/RoaringBitmap-0.5.11.jar with timestamp 1537830708165\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/ST4-4.0.4.jar at spark://172.17.0.13:37187/jars/ST4-4.0.4.jar with timestamp 1537830708165\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/activation-1.1.1.jar at spark://172.17.0.13:37187/jars/activation-1.1.1.jar with timestamp 1537830708165\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/antlr-2.7.7.jar at spark://172.17.0.13:37187/jars/antlr-2.7.7.jar with timestamp 1537830708165\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/antlr-runtime-3.4.jar at spark://172.17.0.13:37187/jars/antlr-runtime-3.4.jar with timestamp 1537830708166\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/antlr4-runtime-4.5.3.jar at spark://172.17.0.13:37187/jars/antlr4-runtime-4.5.3.jar with timestamp 1537830708166\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/aopalliance-1.0.jar at spark://172.17.0.13:37187/jars/aopalliance-1.0.jar with timestamp 1537830708166\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/aopalliance-repackaged-2.4.0-b34.jar at spark://172.17.0.13:37187/jars/aopalliance-repackaged-2.4.0-b34.jar with timestamp 1537830708166\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/apache-log4j-extras-1.2.17.jar at spark://172.17.0.13:37187/jars/apache-log4j-extras-1.2.17.jar with timestamp 1537830708166\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/apacheds-i18n-2.0.0-M15.jar at spark://172.17.0.13:37187/jars/apacheds-i18n-2.0.0-M15.jar with timestamp 1537830708167\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/apacheds-kerberos-codec-2.0.0-M15.jar at spark://172.17.0.13:37187/jars/apacheds-kerberos-codec-2.0.0-M15.jar with timestamp 1537830708167\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/api-asn1-api-1.0.0-M20.jar at spark://172.17.0.13:37187/jars/api-asn1-api-1.0.0-M20.jar with timestamp 1537830708167\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/api-util-1.0.0-M20.jar at spark://172.17.0.13:37187/jars/api-util-1.0.0-M20.jar with timestamp 1537830708167\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/arpack_combined_all-0.1.jar at spark://172.17.0.13:37187/jars/arpack_combined_all-0.1.jar with timestamp 1537830708168\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/avro-1.7.7.jar at spark://172.17.0.13:37187/jars/avro-1.7.7.jar with timestamp 1537830708168\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/avro-ipc-1.7.7.jar at spark://172.17.0.13:37187/jars/avro-ipc-1.7.7.jar with timestamp 1537830708168\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/avro-mapred-1.7.7-hadoop2.jar at spark://172.17.0.13:37187/jars/avro-mapred-1.7.7-hadoop2.jar with timestamp 1537830708168\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/base64-2.3.8.jar at spark://172.17.0.13:37187/jars/base64-2.3.8.jar with timestamp 1537830708168\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/bcprov-jdk15on-1.51.jar at spark://172.17.0.13:37187/jars/bcprov-jdk15on-1.51.jar with timestamp 1537830708168\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/bonecp-0.8.0.RELEASE.jar at spark://172.17.0.13:37187/jars/bonecp-0.8.0.RELEASE.jar with timestamp 1537830708169\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/breeze-macros_2.11-0.13.1.jar at spark://172.17.0.13:37187/jars/breeze-macros_2.11-0.13.1.jar with timestamp 1537830708171\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/breeze_2.11-0.13.1.jar at spark://172.17.0.13:37187/jars/breeze_2.11-0.13.1.jar with timestamp 1537830708172\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/calcite-avatica-1.2.0-incubating.jar at spark://172.17.0.13:37187/jars/calcite-avatica-1.2.0-incubating.jar with timestamp 1537830708173\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/calcite-core-1.2.0-incubating.jar at spark://172.17.0.13:37187/jars/calcite-core-1.2.0-incubating.jar with timestamp 1537830708174\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/calcite-linq4j-1.2.0-incubating.jar at spark://172.17.0.13:37187/jars/calcite-linq4j-1.2.0-incubating.jar with timestamp 1537830708175\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/chill-java-0.8.0.jar at spark://172.17.0.13:37187/jars/chill-java-0.8.0.jar with timestamp 1537830708176\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/chill_2.11-0.8.0.jar at spark://172.17.0.13:37187/jars/chill_2.11-0.8.0.jar with timestamp 1537830708177\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-beanutils-1.7.0.jar at spark://172.17.0.13:37187/jars/commons-beanutils-1.7.0.jar with timestamp 1537830708178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-beanutils-core-1.8.0.jar at spark://172.17.0.13:37187/jars/commons-beanutils-core-1.8.0.jar with timestamp 1537830708180\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-cli-1.2.jar at spark://172.17.0.13:37187/jars/commons-cli-1.2.jar with timestamp 1537830708182\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-codec-1.10.jar at spark://172.17.0.13:37187/jars/commons-codec-1.10.jar with timestamp 1537830708184\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-collections-3.2.2.jar at spark://172.17.0.13:37187/jars/commons-collections-3.2.2.jar with timestamp 1537830708185\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-compiler-3.0.0.jar at spark://172.17.0.13:37187/jars/commons-compiler-3.0.0.jar with timestamp 1537830708187\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-compress-1.4.1.jar at spark://172.17.0.13:37187/jars/commons-compress-1.4.1.jar with timestamp 1537830708188\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-configuration-1.6.jar at spark://172.17.0.13:37187/jars/commons-configuration-1.6.jar with timestamp 1537830708190\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-crypto-1.0.0.jar at spark://172.17.0.13:37187/jars/commons-crypto-1.0.0.jar with timestamp 1537830708192\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-dbcp-1.4.jar at spark://172.17.0.13:37187/jars/commons-dbcp-1.4.jar with timestamp 1537830708194\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-digester-1.8.jar at spark://172.17.0.13:37187/jars/commons-digester-1.8.jar with timestamp 1537830708195\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-httpclient-3.1.jar at spark://172.17.0.13:37187/jars/commons-httpclient-3.1.jar with timestamp 1537830708197\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-io-2.4.jar at spark://172.17.0.13:37187/jars/commons-io-2.4.jar with timestamp 1537830708200\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-lang-2.6.jar at spark://172.17.0.13:37187/jars/commons-lang-2.6.jar with timestamp 1537830708202\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-lang3-3.5.jar at spark://172.17.0.13:37187/jars/commons-lang3-3.5.jar with timestamp 1537830708204\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-logging-1.1.3.jar at spark://172.17.0.13:37187/jars/commons-logging-1.1.3.jar with timestamp 1537830708205\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-math3-3.4.1.jar at spark://172.17.0.13:37187/jars/commons-math3-3.4.1.jar with timestamp 1537830708206\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-net-2.2.jar at spark://172.17.0.13:37187/jars/commons-net-2.2.jar with timestamp 1537830708208\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-pool-1.5.4.jar at spark://172.17.0.13:37187/jars/commons-pool-1.5.4.jar with timestamp 1537830708209\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/compress-lzf-1.0.3.jar at spark://172.17.0.13:37187/jars/compress-lzf-1.0.3.jar with timestamp 1537830708210\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/core-1.1.2.jar at spark://172.17.0.13:37187/jars/core-1.1.2.jar with timestamp 1537830708212\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/curator-client-2.6.0.jar at spark://172.17.0.13:37187/jars/curator-client-2.6.0.jar with timestamp 1537830708213\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/curator-framework-2.6.0.jar at spark://172.17.0.13:37187/jars/curator-framework-2.6.0.jar with timestamp 1537830708217\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/curator-recipes-2.6.0.jar at spark://172.17.0.13:37187/jars/curator-recipes-2.6.0.jar with timestamp 1537830708219\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/datanucleus-api-jdo-3.2.6.jar at spark://172.17.0.13:37187/jars/datanucleus-api-jdo-3.2.6.jar with timestamp 1537830708221\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/datanucleus-core-3.2.10.jar at spark://172.17.0.13:37187/jars/datanucleus-core-3.2.10.jar with timestamp 1537830708222\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/datanucleus-rdbms-3.2.9.jar at spark://172.17.0.13:37187/jars/datanucleus-rdbms-3.2.9.jar with timestamp 1537830708223\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/derby-10.12.1.1.jar at spark://172.17.0.13:37187/jars/derby-10.12.1.1.jar with timestamp 1537830708226\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/eigenbase-properties-1.1.5.jar at spark://172.17.0.13:37187/jars/eigenbase-properties-1.1.5.jar with timestamp 1537830708227\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/gson-2.2.4.jar at spark://172.17.0.13:37187/jars/gson-2.2.4.jar with timestamp 1537830708229\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/guava-14.0.1.jar at spark://172.17.0.13:37187/jars/guava-14.0.1.jar with timestamp 1537830708230\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/guice-3.0.jar at spark://172.17.0.13:37187/jars/guice-3.0.jar with timestamp 1537830708232\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/guice-servlet-3.0.jar at spark://172.17.0.13:37187/jars/guice-servlet-3.0.jar with timestamp 1537830708233\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-annotations-2.7.3.jar at spark://172.17.0.13:37187/jars/hadoop-annotations-2.7.3.jar with timestamp 1537830708235\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-auth-2.7.3.jar at spark://172.17.0.13:37187/jars/hadoop-auth-2.7.3.jar with timestamp 1537830708235\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-client-2.7.3.jar at spark://172.17.0.13:37187/jars/hadoop-client-2.7.3.jar with timestamp 1537830708236\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-common-2.7.3.jar at spark://172.17.0.13:37187/jars/hadoop-common-2.7.3.jar with timestamp 1537830708237\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-hdfs-2.7.3.jar at spark://172.17.0.13:37187/jars/hadoop-hdfs-2.7.3.jar with timestamp 1537830708238\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-mapreduce-client-app-2.7.3.jar at spark://172.17.0.13:37187/jars/hadoop-mapreduce-client-app-2.7.3.jar with timestamp 1537830708240\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-mapreduce-client-common-2.7.3.jar at spark://172.17.0.13:37187/jars/hadoop-mapreduce-client-common-2.7.3.jar with timestamp 1537830708240\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-mapreduce-client-core-2.7.3.jar at spark://172.17.0.13:37187/jars/hadoop-mapreduce-client-core-2.7.3.jar with timestamp 1537830708241\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar at spark://172.17.0.13:37187/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar with timestamp 1537830708243\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar at spark://172.17.0.13:37187/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar with timestamp 1537830708245\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-yarn-api-2.7.3.jar at spark://172.17.0.13:37187/jars/hadoop-yarn-api-2.7.3.jar with timestamp 1537830708245\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-yarn-client-2.7.3.jar at spark://172.17.0.13:37187/jars/hadoop-yarn-client-2.7.3.jar with timestamp 1537830708250\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-yarn-common-2.7.3.jar at spark://172.17.0.13:37187/jars/hadoop-yarn-common-2.7.3.jar with timestamp 1537830708251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-yarn-server-common-2.7.3.jar at spark://172.17.0.13:37187/jars/hadoop-yarn-server-common-2.7.3.jar with timestamp 1537830708252\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-yarn-server-web-proxy-2.7.3.jar at spark://172.17.0.13:37187/jars/hadoop-yarn-server-web-proxy-2.7.3.jar with timestamp 1537830708252\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hive-beeline-1.2.1.spark2.jar at spark://172.17.0.13:37187/jars/hive-beeline-1.2.1.spark2.jar with timestamp 1537830708253\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hive-cli-1.2.1.spark2.jar at spark://172.17.0.13:37187/jars/hive-cli-1.2.1.spark2.jar with timestamp 1537830708254\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hive-exec-1.2.1.spark2.jar at spark://172.17.0.13:37187/jars/hive-exec-1.2.1.spark2.jar with timestamp 1537830708255\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hive-jdbc-1.2.1.spark2.jar at spark://172.17.0.13:37187/jars/hive-jdbc-1.2.1.spark2.jar with timestamp 1537830708256\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hive-metastore-1.2.1.spark2.jar at spark://172.17.0.13:37187/jars/hive-metastore-1.2.1.spark2.jar with timestamp 1537830708256\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hk2-api-2.4.0-b34.jar at spark://172.17.0.13:37187/jars/hk2-api-2.4.0-b34.jar with timestamp 1537830708258\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hk2-locator-2.4.0-b34.jar at spark://172.17.0.13:37187/jars/hk2-locator-2.4.0-b34.jar with timestamp 1537830708259\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/hk2-utils-2.4.0-b34.jar at spark://172.17.0.13:37187/jars/hk2-utils-2.4.0-b34.jar with timestamp 1537830708260\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/htrace-core-3.1.0-incubating.jar at spark://172.17.0.13:37187/jars/htrace-core-3.1.0-incubating.jar with timestamp 1537830708262\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/httpclient-4.5.2.jar at spark://172.17.0.13:37187/jars/httpclient-4.5.2.jar with timestamp 1537830708267\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/httpcore-4.4.4.jar at spark://172.17.0.13:37187/jars/httpcore-4.4.4.jar with timestamp 1537830708268\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/ivy-2.4.0.jar at spark://172.17.0.13:37187/jars/ivy-2.4.0.jar with timestamp 1537830708268\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jackson-annotations-2.6.5.jar at spark://172.17.0.13:37187/jars/jackson-annotations-2.6.5.jar with timestamp 1537830708269\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jackson-core-2.6.5.jar at spark://172.17.0.13:37187/jars/jackson-core-2.6.5.jar with timestamp 1537830708270\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jackson-core-asl-1.9.13.jar at spark://172.17.0.13:37187/jars/jackson-core-asl-1.9.13.jar with timestamp 1537830708273\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jackson-databind-2.6.5.jar at spark://172.17.0.13:37187/jars/jackson-databind-2.6.5.jar with timestamp 1537830708274\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jackson-jaxrs-1.9.13.jar at spark://172.17.0.13:37187/jars/jackson-jaxrs-1.9.13.jar with timestamp 1537830708276\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jackson-mapper-asl-1.9.13.jar at spark://172.17.0.13:37187/jars/jackson-mapper-asl-1.9.13.jar with timestamp 1537830708277\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jackson-module-paranamer-2.6.5.jar at spark://172.17.0.13:37187/jars/jackson-module-paranamer-2.6.5.jar with timestamp 1537830708279\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jackson-module-scala_2.11-2.6.5.jar at spark://172.17.0.13:37187/jars/jackson-module-scala_2.11-2.6.5.jar with timestamp 1537830708280\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jackson-xc-1.9.13.jar at spark://172.17.0.13:37187/jars/jackson-xc-1.9.13.jar with timestamp 1537830708282\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/janino-3.0.0.jar at spark://172.17.0.13:37187/jars/janino-3.0.0.jar with timestamp 1537830708284\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/java-xmlbuilder-1.0.jar at spark://172.17.0.13:37187/jars/java-xmlbuilder-1.0.jar with timestamp 1537830708284\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/javassist-3.18.1-GA.jar at spark://172.17.0.13:37187/jars/javassist-3.18.1-GA.jar with timestamp 1537830708286\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/javax.annotation-api-1.2.jar at spark://172.17.0.13:37187/jars/javax.annotation-api-1.2.jar with timestamp 1537830708287\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/javax.inject-1.jar at spark://172.17.0.13:37187/jars/javax.inject-1.jar with timestamp 1537830708288\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/javax.inject-2.4.0-b34.jar at spark://172.17.0.13:37187/jars/javax.inject-2.4.0-b34.jar with timestamp 1537830708289\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/javax.servlet-api-3.1.0.jar at spark://172.17.0.13:37187/jars/javax.servlet-api-3.1.0.jar with timestamp 1537830708290\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/javax.ws.rs-api-2.0.1.jar at spark://172.17.0.13:37187/jars/javax.ws.rs-api-2.0.1.jar with timestamp 1537830708291\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/javolution-5.5.1.jar at spark://172.17.0.13:37187/jars/javolution-5.5.1.jar with timestamp 1537830708291\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jaxb-api-2.2.2.jar at spark://172.17.0.13:37187/jars/jaxb-api-2.2.2.jar with timestamp 1537830708292\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jcl-over-slf4j-1.7.16.jar at spark://172.17.0.13:37187/jars/jcl-over-slf4j-1.7.16.jar with timestamp 1537830708293\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jdo-api-3.0.1.jar at spark://172.17.0.13:37187/jars/jdo-api-3.0.1.jar with timestamp 1537830708294\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jersey-client-2.22.2.jar at spark://172.17.0.13:37187/jars/jersey-client-2.22.2.jar with timestamp 1537830708295\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jersey-common-2.22.2.jar at spark://172.17.0.13:37187/jars/jersey-common-2.22.2.jar with timestamp 1537830708296\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jersey-container-servlet-2.22.2.jar at spark://172.17.0.13:37187/jars/jersey-container-servlet-2.22.2.jar with timestamp 1537830708298\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jersey-container-servlet-core-2.22.2.jar at spark://172.17.0.13:37187/jars/jersey-container-servlet-core-2.22.2.jar with timestamp 1537830708301\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jersey-guava-2.22.2.jar at spark://172.17.0.13:37187/jars/jersey-guava-2.22.2.jar with timestamp 1537830708302\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jersey-media-jaxb-2.22.2.jar at spark://172.17.0.13:37187/jars/jersey-media-jaxb-2.22.2.jar with timestamp 1537830708303\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jersey-server-2.22.2.jar at spark://172.17.0.13:37187/jars/jersey-server-2.22.2.jar with timestamp 1537830708304\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jets3t-0.9.3.jar at spark://172.17.0.13:37187/jars/jets3t-0.9.3.jar with timestamp 1537830708305\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jetty-6.1.26.jar at spark://172.17.0.13:37187/jars/jetty-6.1.26.jar with timestamp 1537830708307\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jetty-util-6.1.26.jar at spark://172.17.0.13:37187/jars/jetty-util-6.1.26.jar with timestamp 1537830708311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jline-2.12.1.jar at spark://172.17.0.13:37187/jars/jline-2.12.1.jar with timestamp 1537830708312\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/joda-time-2.9.3.jar at spark://172.17.0.13:37187/jars/joda-time-2.9.3.jar with timestamp 1537830708313\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jodd-core-3.5.2.jar at spark://172.17.0.13:37187/jars/jodd-core-3.5.2.jar with timestamp 1537830708313\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jpam-1.1.jar at spark://172.17.0.13:37187/jars/jpam-1.1.jar with timestamp 1537830708314\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/json4s-ast_2.11-3.2.11.jar at spark://172.17.0.13:37187/jars/json4s-ast_2.11-3.2.11.jar with timestamp 1537830708315\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/json4s-core_2.11-3.2.11.jar at spark://172.17.0.13:37187/jars/json4s-core_2.11-3.2.11.jar with timestamp 1537830708316\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/json4s-jackson_2.11-3.2.11.jar at spark://172.17.0.13:37187/jars/json4s-jackson_2.11-3.2.11.jar with timestamp 1537830708318\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jsp-api-2.1.jar at spark://172.17.0.13:37187/jars/jsp-api-2.1.jar with timestamp 1537830708321\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jsr305-1.3.9.jar at spark://172.17.0.13:37187/jars/jsr305-1.3.9.jar with timestamp 1537830708323\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jta-1.1.jar at spark://172.17.0.13:37187/jars/jta-1.1.jar with timestamp 1537830708325\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jtransforms-2.4.0.jar at spark://172.17.0.13:37187/jars/jtransforms-2.4.0.jar with timestamp 1537830708326\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/jul-to-slf4j-1.7.16.jar at spark://172.17.0.13:37187/jars/jul-to-slf4j-1.7.16.jar with timestamp 1537830708328\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/kryo-shaded-3.0.3.jar at spark://172.17.0.13:37187/jars/kryo-shaded-3.0.3.jar with timestamp 1537830708330\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/leveldbjni-all-1.8.jar at spark://172.17.0.13:37187/jars/leveldbjni-all-1.8.jar with timestamp 1537830708333\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/libfb303-0.9.3.jar at spark://172.17.0.13:37187/jars/libfb303-0.9.3.jar with timestamp 1537830708335\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/libthrift-0.9.3.jar at spark://172.17.0.13:37187/jars/libthrift-0.9.3.jar with timestamp 1537830708337\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/log4j-1.2.17.jar at spark://172.17.0.13:37187/jars/log4j-1.2.17.jar with timestamp 1537830708338\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/lz4-1.3.0.jar at spark://172.17.0.13:37187/jars/lz4-1.3.0.jar with timestamp 1537830708339\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/machinist_2.11-0.6.1.jar at spark://172.17.0.13:37187/jars/machinist_2.11-0.6.1.jar with timestamp 1537830708339\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/macro-compat_2.11-1.1.1.jar at spark://172.17.0.13:37187/jars/macro-compat_2.11-1.1.1.jar with timestamp 1537830708340\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/mail-1.4.7.jar at spark://172.17.0.13:37187/jars/mail-1.4.7.jar with timestamp 1537830708341\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/mesos-1.0.0-shaded-protobuf.jar at spark://172.17.0.13:37187/jars/mesos-1.0.0-shaded-protobuf.jar with timestamp 1537830708342\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/metrics-core-3.1.2.jar at spark://172.17.0.13:37187/jars/metrics-core-3.1.2.jar with timestamp 1537830708344\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/metrics-graphite-3.1.2.jar at spark://172.17.0.13:37187/jars/metrics-graphite-3.1.2.jar with timestamp 1537830708346\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/metrics-json-3.1.2.jar at spark://172.17.0.13:37187/jars/metrics-json-3.1.2.jar with timestamp 1537830708347\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/metrics-jvm-3.1.2.jar at spark://172.17.0.13:37187/jars/metrics-jvm-3.1.2.jar with timestamp 1537830708348\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/minlog-1.3.0.jar at spark://172.17.0.13:37187/jars/minlog-1.3.0.jar with timestamp 1537830708351\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/mx4j-3.0.2.jar at spark://172.17.0.13:37187/jars/mx4j-3.0.2.jar with timestamp 1537830708353\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/netty-3.9.9.Final.jar at spark://172.17.0.13:37187/jars/netty-3.9.9.Final.jar with timestamp 1537830708355\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/netty-all-4.0.43.Final.jar at spark://172.17.0.13:37187/jars/netty-all-4.0.43.Final.jar with timestamp 1537830708357\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/objenesis-2.1.jar at spark://172.17.0.13:37187/jars/objenesis-2.1.jar with timestamp 1537830708358\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/opencsv-2.3.jar at spark://172.17.0.13:37187/jars/opencsv-2.3.jar with timestamp 1537830708360\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/oro-2.0.8.jar at spark://172.17.0.13:37187/jars/oro-2.0.8.jar with timestamp 1537830708361\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/osgi-resource-locator-1.0.1.jar at spark://172.17.0.13:37187/jars/osgi-resource-locator-1.0.1.jar with timestamp 1537830708361\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/paranamer-2.6.jar at spark://172.17.0.13:37187/jars/paranamer-2.6.jar with timestamp 1537830708362\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/parquet-column-1.8.2.jar at spark://172.17.0.13:37187/jars/parquet-column-1.8.2.jar with timestamp 1537830708362\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/parquet-common-1.8.2.jar at spark://172.17.0.13:37187/jars/parquet-common-1.8.2.jar with timestamp 1537830708364\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/parquet-encoding-1.8.2.jar at spark://172.17.0.13:37187/jars/parquet-encoding-1.8.2.jar with timestamp 1537830708366\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/parquet-format-2.3.1.jar at spark://172.17.0.13:37187/jars/parquet-format-2.3.1.jar with timestamp 1537830708368\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/parquet-hadoop-1.8.2.jar at spark://172.17.0.13:37187/jars/parquet-hadoop-1.8.2.jar with timestamp 1537830708369\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/parquet-hadoop-bundle-1.6.0.jar at spark://172.17.0.13:37187/jars/parquet-hadoop-bundle-1.6.0.jar with timestamp 1537830708370\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/parquet-jackson-1.8.2.jar at spark://172.17.0.13:37187/jars/parquet-jackson-1.8.2.jar with timestamp 1537830708371\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/pmml-model-1.2.15.jar at spark://172.17.0.13:37187/jars/pmml-model-1.2.15.jar with timestamp 1537830708373\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/pmml-schema-1.2.15.jar at spark://172.17.0.13:37187/jars/pmml-schema-1.2.15.jar with timestamp 1537830708374\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/protobuf-java-2.5.0.jar at spark://172.17.0.13:37187/jars/protobuf-java-2.5.0.jar with timestamp 1537830708375\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/py4j-0.10.4.jar at spark://172.17.0.13:37187/jars/py4j-0.10.4.jar with timestamp 1537830708378\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/pyrolite-4.13.jar at spark://172.17.0.13:37187/jars/pyrolite-4.13.jar with timestamp 1537830708378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/scala-compiler-2.11.8.jar at spark://172.17.0.13:37187/jars/scala-compiler-2.11.8.jar with timestamp 1537830708379\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/scala-library-2.11.8.jar at spark://172.17.0.13:37187/jars/scala-library-2.11.8.jar with timestamp 1537830708379\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/scala-parser-combinators_2.11-1.0.4.jar at spark://172.17.0.13:37187/jars/scala-parser-combinators_2.11-1.0.4.jar with timestamp 1537830708380\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/scala-reflect-2.11.8.jar at spark://172.17.0.13:37187/jars/scala-reflect-2.11.8.jar with timestamp 1537830708383\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/scala-xml_2.11-1.0.2.jar at spark://172.17.0.13:37187/jars/scala-xml_2.11-1.0.2.jar with timestamp 1537830708386\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/scalap-2.11.8.jar at spark://172.17.0.13:37187/jars/scalap-2.11.8.jar with timestamp 1537830708388\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/shapeless_2.11-2.3.2.jar at spark://172.17.0.13:37187/jars/shapeless_2.11-2.3.2.jar with timestamp 1537830708388\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/slf4j-api-1.7.16.jar at spark://172.17.0.13:37187/jars/slf4j-api-1.7.16.jar with timestamp 1537830708389\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/slf4j-log4j12-1.7.16.jar at spark://172.17.0.13:37187/jars/slf4j-log4j12-1.7.16.jar with timestamp 1537830708390\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/snappy-0.2.jar at spark://172.17.0.13:37187/jars/snappy-0.2.jar with timestamp 1537830708391\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/snappy-java-1.1.2.6.jar at spark://172.17.0.13:37187/jars/snappy-java-1.1.2.6.jar with timestamp 1537830708392\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-catalyst_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-catalyst_2.11-2.2.0.jar with timestamp 1537830708392\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-core_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-core_2.11-2.2.0.jar with timestamp 1537830708393\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-graphx_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-graphx_2.11-2.2.0.jar with timestamp 1537830708394\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-hive-thriftserver_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-hive-thriftserver_2.11-2.2.0.jar with timestamp 1537830708396\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-hive_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-hive_2.11-2.2.0.jar with timestamp 1537830708399\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-launcher_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-launcher_2.11-2.2.0.jar with timestamp 1537830708400\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-mesos_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-mesos_2.11-2.2.0.jar with timestamp 1537830708402\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-mllib-local_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-mllib-local_2.11-2.2.0.jar with timestamp 1537830708403\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-mllib_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-mllib_2.11-2.2.0.jar with timestamp 1537830708404\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-network-common_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-network-common_2.11-2.2.0.jar with timestamp 1537830708405\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-network-shuffle_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-network-shuffle_2.11-2.2.0.jar with timestamp 1537830708406\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-repl_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-repl_2.11-2.2.0.jar with timestamp 1537830708407\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-sketch_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-sketch_2.11-2.2.0.jar with timestamp 1537830708407\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-sql_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-sql_2.11-2.2.0.jar with timestamp 1537830708408\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-streaming_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-streaming_2.11-2.2.0.jar with timestamp 1537830708408\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-tags_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-tags_2.11-2.2.0.jar with timestamp 1537830708409\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-unsafe_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-unsafe_2.11-2.2.0.jar with timestamp 1537830708409\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-yarn_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-yarn_2.11-2.2.0.jar with timestamp 1537830708410\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spire-macros_2.11-0.13.0.jar at spark://172.17.0.13:37187/jars/spire-macros_2.11-0.13.0.jar with timestamp 1537830708410\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/spire_2.11-0.13.0.jar at spark://172.17.0.13:37187/jars/spire_2.11-0.13.0.jar with timestamp 1537830708411\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/stax-api-1.0-2.jar at spark://172.17.0.13:37187/jars/stax-api-1.0-2.jar with timestamp 1537830708412\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/stax-api-1.0.1.jar at spark://172.17.0.13:37187/jars/stax-api-1.0.1.jar with timestamp 1537830708413\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/stream-2.7.0.jar at spark://172.17.0.13:37187/jars/stream-2.7.0.jar with timestamp 1537830708416\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/stringtemplate-3.2.1.jar at spark://172.17.0.13:37187/jars/stringtemplate-3.2.1.jar with timestamp 1537830708417\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/super-csv-2.2.0.jar at spark://172.17.0.13:37187/jars/super-csv-2.2.0.jar with timestamp 1537830708417\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/univocity-parsers-2.2.1.jar at spark://172.17.0.13:37187/jars/univocity-parsers-2.2.1.jar with timestamp 1537830708417\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/validation-api-1.1.0.Final.jar at spark://172.17.0.13:37187/jars/validation-api-1.1.0.Final.jar with timestamp 1537830708420\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/xbean-asm5-shaded-4.4.jar at spark://172.17.0.13:37187/jars/xbean-asm5-shaded-4.4.jar with timestamp 1537830708421\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/xercesImpl-2.9.1.jar at spark://172.17.0.13:37187/jars/xercesImpl-2.9.1.jar with timestamp 1537830708422\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/xmlenc-0.52.jar at spark://172.17.0.13:37187/jars/xmlenc-0.52.jar with timestamp 1537830708423\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/xz-1.0.jar at spark://172.17.0.13:37187/jars/xz-1.0.jar with timestamp 1537830708425\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/jars/zookeeper-3.4.6.jar at spark://172.17.0.13:37187/jars/zookeeper-3.4.6.jar with timestamp 1537830708426\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/kafka/kafka-clients/0.10.0.1/kafka-clients-0.10.0.1.jar at spark://172.17.0.13:37187/jars/kafka-clients-0.10.0.1.jar with timestamp 1537830708428\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.11/2.2.0/spark-sql-kafka-0-10_2.11-2.2.0.jar at spark://172.17.0.13:37187/jars/spark-sql-kafka-0-10_2.11-2.2.0.jar with timestamp 1537830708429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.21/slf4j-api-1.7.21.jar at spark://172.17.0.13:37187/jars/slf4j-api-1.7.21.jar with timestamp 1537830708430\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/isarnproject/isarn-sketches-spark_2.11/0.3.1-sp2.2-py2.7/isarn-sketches-spark_2.11-0.3.1-sp2.2-py2.7.jar at spark://172.17.0.13:37187/jars/isarn-sketches-spark_2.11-0.3.1-sp2.2-py2.7.jar with timestamp 1537830708433\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/isarnproject/isarn-sketches_2.11/0.1.2/isarn-sketches_2.11-0.1.2.jar at spark://172.17.0.13:37187/jars/isarn-sketches_2.11-0.1.2.jar with timestamp 1537830708435\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/isarnproject/isarn-algebra-api_2.11/0.0.3/isarn-algebra-api_2.11-0.0.3.jar at spark://172.17.0.13:37187/jars/isarn-algebra-api_2.11-0.0.3.jar with timestamp 1537830708437\n",
      "18/09/24 23:11:48 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/isarnproject/isarn-collections_2.11/0.0.4/isarn-collections_2.11-0.0.4.jar at spark://172.17.0.13:37187/jars/isarn-collections_2.11-0.0.4.jar with timestamp 1537830708438\n",
      "18/09/24 23:11:48 INFO Executor: Starting executor ID driver on host localhost\n",
      "18/09/24 23:11:48 INFO Executor: Using REPL class URI: http://172.17.0.13:33275\n",
      "18/09/24 23:11:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33525.\n",
      "18/09/24 23:11:48 INFO NettyBlockTransferService: Server created on 172.17.0.13:33525\n",
      "18/09/24 23:11:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "18/09/24 23:11:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.17.0.13, 33525, None)\n",
      "18/09/24 23:11:48 INFO BlockManagerMasterEndpoint: Registering block manager 172.17.0.13:33525 with 1909.8 MB RAM, BlockManagerId(driver, 172.17.0.13, 33525, None)\n",
      "18/09/24 23:11:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.17.0.13, 33525, None)\n",
      "18/09/24 23:11:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.17.0.13, 33525, None)\n",
      "18/09/24 23:11:48 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/notebooks/spark-warehouse/').\n",
      "18/09/24 23:11:48 INFO SharedState: Warehouse path is 'file:/notebooks/spark-warehouse/'.\n",
      "18/09/24 23:11:49 WARN SharedState: URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory\n",
      "18/09/24 23:11:49 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@3e3bc64f"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "val spark = {\n",
    "    AmmoniteSparkSession.builder()\n",
    "      .master(\"local[2]\")\n",
    "      .getOrCreate()\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mappender\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mlog4j\u001b[39m.\u001b[32mConsoleAppender\u001b[39m = org.apache.log4j.ConsoleAppender@5b9385d7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val appender = org.apache.log4j.Logger.getRootLogger().getAppender(\"console\").asInstanceOf[org.apache.log4j.ConsoleAppender]\n",
    "appender.setThreshold(org.apache.log4j.Level.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [key: binary, value: binary ... 5 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark\n",
    "  .read\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "  .option(\"subscribe\", \"social-firehose\")\n",
    "  .load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.sqlContext.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import spark.sqlContext.implicits._\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mvalues\u001b[39m: \u001b[32mDataFrame\u001b[39m = [value: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val values = df.select(($\"value\").cast(StringType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres8_0\u001b[39m: \u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(\n",
       "  <function1>,\n",
       "  LongType,\n",
       "  \u001b[33mSome\u001b[39m(\u001b[33mList\u001b[39m(StringType))\n",
       ")\n",
       "\u001b[36mres8_1\u001b[39m: \u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(\n",
       "  <function1>,\n",
       "  IntegerType,\n",
       "  \u001b[33mSome\u001b[39m(\u001b[33mList\u001b[39m(StringType))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"utime\", (x:String)=>x.toLong / 1000)\n",
    "spark.udf.register(\"wordcount\", (text: String)=>text.split(\" \").filter(_.length > 0).length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mstructure\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"text\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"user_id\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"update_id\"\u001b[39m, StringType, true, {})\n",
       ")\n",
       "\u001b[36mrecords\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [user_id: string, utime: bigint ... 1 more field]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val structure = StructType(Seq(\"text\",\"user_id\",\"update_id\").map{f=>StructField(f, StringType, true)})\n",
    "val records = values.select(from_json($\"value\", structure).alias(\"json\"))\n",
    "    .select($\"json.update_id\", $\"json.user_id\", $\"json.text\")\n",
    "    .select($\"user_id\",\n",
    "            callUDF(\"utime\", $\"update_id\").alias(\"utime\"),\n",
    "            callUDF(\"wordcount\", $\"text\").alias(\"wordcount\"))\n",
    "    .filter($\"utime\" > 220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "val times = records.withColumn(\"time\",current_timestamp())\n",
    "val w = times.groupBy(window($\"time\",\"1 second\"), $\"user_id\").count()\n",
    "val query = w.writeStream\n",
    "  .outputMode(\"complete\")\n",
    "  .format(\"console\")\n",
    "  .start()\n",
    "query.awaitTermination\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "val wspec = Window.partitionBy(\"utime\").rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "val w = records.withColumn(\"max\", max($\"user_id\").over(wspec))\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+------------------+\n",
      "|utime|max|min|               avg|\n",
      "+-----+---+---+------------------+\n",
      "|  230| 46|  3|18.507246376811594|\n",
      "|  229| 44|  2|            17.424|\n",
      "|  228| 44|  2|            17.671|\n",
      "|  227| 44|  1|            17.327|\n",
      "|  226| 46|  2|            18.232|\n",
      "+-----+---+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mt\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [utime: bigint, max: int ... 2 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t = records.groupBy($\"utime\")\n",
    "    .agg(max($\"wordcount\").alias(\"max\"),\n",
    "         min($\"wordcount\").alias(\"min\"),\n",
    "         avg($\"wordcount\").alias(\"avg\"))\n",
    "    .orderBy($\"utime\".desc)\n",
    "t.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.isarnproject.sketches._, org.isarnproject.sketches.udaf._, org.apache.spark.isarnproject.sketches.udt._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random.nextGaussian\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.isarnproject.sketches._, org.isarnproject.sketches.udaf._, org.apache.spark.isarnproject.sketches.udt._\n",
    "import scala.util.Random.nextGaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msketchDistribution\u001b[39m: \u001b[32mTDigestUDAF\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mTDigestUDAF\u001b[39m(\u001b[32m0.2\u001b[39m, \u001b[32m25\u001b[39m)\n",
       "\u001b[36mres14_1\u001b[39m: \u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(\n",
       "  <function1>,\n",
       "  DoubleType,\n",
       "  \u001b[32mNone\u001b[39m\n",
       ")\n",
       "\u001b[36mres14_2\u001b[39m: \u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(\n",
       "  <function1>,\n",
       "  DoubleType,\n",
       "  \u001b[32mNone\u001b[39m\n",
       ")\n",
       "\u001b[36mres14_3\u001b[39m: \u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(\n",
       "  <function1>,\n",
       "  DoubleType,\n",
       "  \u001b[32mNone\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sketchDistribution = tdigestUDAF[Double].delta(0.2).maxDiscrete(25)\n",
    "spark.udf.register(\"p50\", (c:Any)=>c.asInstanceOf[TDigestSQL].tdigest.cdfInverse(0.5))\n",
    "spark.udf.register(\"p90\", (c:Any)=>c.asInstanceOf[TDigestSQL].tdigest.cdfInverse(0.9))\n",
    "spark.udf.register(\"p99\", (c:Any)=>c.asInstanceOf[TDigestSQL].tdigest.cdfInverse(0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+---+------------------+\n",
      "|utime|              sketch|min|max|               avg|\n",
      "+-----+--------------------+---+---+------------------+\n",
      "|  230|TDigestSQL(TDiges...|  3| 46|18.507246376811594|\n",
      "|  229|TDigestSQL(TDiges...|  2| 44|            17.424|\n",
      "|  228|TDigestSQL(TDiges...|  2| 44|            17.671|\n",
      "|  227|TDigestSQL(TDiges...|  1| 44|            17.327|\n",
      "|  226|TDigestSQL(TDiges...|  2| 46|            18.232|\n",
      "+-----+--------------------+---+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mt\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [utime: bigint, sketch: tdigest ... 3 more fields]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t = records.groupBy($\"utime\")\n",
    "    .agg(sketchDistribution($\"wordcount\").alias(\"sketch\"),\n",
    "         min($\"wordcount\").alias(\"min\"),\n",
    "         max($\"wordcount\").alias(\"max\"),\n",
    "         avg($\"wordcount\").alias(\"avg\"))\n",
    "    .orderBy($\"utime\".desc)\n",
    "t.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+------------------+------------------+------------------+------------------+\n",
      "|utime|min|max|               avg|               p50|               p90|               p99|\n",
      "+-----+---+---+------------------+------------------+------------------+------------------+\n",
      "|  230|  3| 46|18.507246376811594|              15.0|              31.0|44.620000000000005|\n",
      "|  229|  2| 44|            17.424|15.767441860465116|              31.0|            38.875|\n",
      "|  228|  2| 44|            17.671|15.808988764044944|30.031992563804113|              39.0|\n",
      "|  227|  1| 44|            17.327|              15.0|              30.0|              39.0|\n",
      "|  226|  2| 46|            18.232|              16.0|31.885714285714286|              39.0|\n",
      "|  225|  1| 41|            16.897|              15.0|29.949291631157948| 37.61538461538461|\n",
      "|  224|  1| 44|            18.218|16.817073170731707|              32.0|              39.0|\n",
      "|  223|  2| 49|            17.546|15.596012010611316|              31.0|              39.0|\n",
      "|  222|  2| 44|            17.572|15.605821947611172|              31.0|             39.75|\n",
      "|  221|  1| 46|            17.415| 15.56338028169014|              30.0|              40.0|\n",
      "+-----+---+---+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mt2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [utime: bigint, min: int ... 5 more fields]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t2 = t.select(\n",
    "    $\"utime\", $\"min\", $\"max\", $\"avg\",\n",
    "    callUDF(\"p50\", $\"sketch\").alias(\"p50\"),\n",
    "    callUDF(\"p90\", $\"sketch\").alias(\"p90\"),\n",
    "    callUDF(\"p99\", $\"sketch\").alias(\"p99\")\n",
    ")\n",
    "t2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "object topk {\n",
    "import org.apache.spark.util.sketch.CountMinSketch\n",
    "\n",
    "class TopK[V] ( \n",
    "  val k: Int,\n",
    "  val cms: CountMinSketch,\n",
    "  val topk: Vector[(V, Int)],\n",
    "  val seed: Int) extends Serializable {\n",
    "\n",
    "  // mmmm, sugary...\n",
    "  def confidence = cms.confidence()\n",
    "  def epsilon = cms.relativeError()\n",
    "\n",
    "  // update the TopK sketch w/ a new element 'v'\n",
    "  def +(v: V): TopK[V] = {\n",
    "    val ecms: CountMinSketch = CountMinSketch.create(epsilon, confidence, seed)\n",
    "    val ucms: CountMinSketch = ecms.mergeInPlace(this.cms)\n",
    "    ucms.add(v, 1)\n",
    "    val f = ucms.estimateCount(v).toInt\n",
    "    val utopk = {\n",
    "      // search through the current top-k table.\n",
    "      // If v is already in the table, its recorded frequency will be < f, due to the\n",
    "      // way that CMS estimates object frequencies.\n",
    "      // Stop if we hit a frequency >= f, or if we find value v already in the table.\n",
    "      val j = topk.indexWhere { case (tv, tf) => (tf >= f) || (v == tv) }\n",
    "      if (j == -1) {\n",
    "        // v is not present, and f is > all current frequencies. Add to the end.\n",
    "        topk :+ (v, f)\n",
    "      } else {\n",
    "        val (jv, jf) = topk(j)\n",
    "        if (j == 0 && f <= jf && topk.length == k) {\n",
    "          // (v,f) doesn't fall into the current top-k, so no change\n",
    "          topk\n",
    "        } else if (v == jv) {\n",
    "          // value 'v' already exists in the top-k, so update its frequency\n",
    "          if ((j < topk.length - 1) && (f > topk(j + 1)._2)) {\n",
    "            // (v, f) needs to be reordered, just re-sort it\n",
    "            topk.updated(j, (v, f)).sortBy(_._2)\n",
    "          } else {\n",
    "            topk.updated(j, (v, f))\n",
    "          }\n",
    "        } else {\n",
    "          // v is new, and has a place in the top k, so insert it\n",
    "          topk.patch(j, Vector((v, f)), 0)\n",
    "        }\n",
    "      }\n",
    "    }.takeRight(k)\n",
    "    new TopK[V](k, ucms, utopk, seed)\n",
    "  }\n",
    "\n",
    "  // combine two TopK sketches, monoidally\n",
    "  def ++(that: TopK[V]): TopK[V] = {\n",
    "    val ecms: CountMinSketch = CountMinSketch.create(epsilon, confidence, seed)\n",
    "    val thatcms = ecms.mergeInPlace(that.cms) \n",
    "    val ucms = thatcms.mergeInPlace(this.cms)\n",
    "    val utopk =\n",
    "      (this.topk ++ that.topk)\n",
    "        .map { case (v, _) => v }\n",
    "        .distinct\n",
    "        .map { v => (v, ucms.estimateCount(v).toInt) }\n",
    "        .sortBy { case (_, f) => f }\n",
    "        .takeRight(k)\n",
    "    new TopK[V](k, ucms, utopk, seed)\n",
    "  }\n",
    "\n",
    "  def top(n: Int): Vector[(V, Int)] = this.topk.reverse.take(n)\n",
    "\n",
    "  override def toString: String = \"[\" + top(k).foldLeft(\"\")((a, b) => a + s\", ${b._1} ~${b._2}\").drop(2) + \"]\"\n",
    "}\n",
    "\n",
    "// eps, confidence, seed\n",
    "object TopK {\n",
    "  def epsilonDefault = 0.01\n",
    "  def confidenceDefault = 0.99\n",
    "  def seedDefault = 13 //scala.util.Random.nextInt()\n",
    "\n",
    "  def empty[V](k: Int,\n",
    "      epsilon: Double = epsilonDefault,\n",
    "      confidence: Double = confidenceDefault,\n",
    "      seed: Int = seedDefault) =\n",
    "    new TopK[V](k, CountMinSketch.create(epsilon, confidence, seed), Vector.empty[(V, Int)], seed)\n",
    "}\n",
    "}\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "object topKAgg {\n",
    "class TopKAgg[I](f: I => String) extends Aggregator[I, topk.TopK[String], Array[(String,Int)]] {\n",
    "    def zero = topk.TopK.empty[String](5)\n",
    "    def reduce(b: topk.TopK[String], a: I) = b + f(a)\n",
    "    def merge(b1: topk.TopK[String], b2: topk.TopK[String]) = b1 ++ b2\n",
    "    def finish(r: topk.TopK[String]) = r.top(5).toArray\n",
    "    val bufferEncoder = Encoders.kryo[topk.TopK[String]]\n",
    "    val outputEncoder = Encoders.kryo[Array[(String,Int)]]\n",
    "}\n",
    "def tka = new TopKAgg[(Long,String,String)](_._3)\n",
    "}\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------------+\n",
      "|utime|   user_id|             word|\n",
      "+-----+----------+-----------------+\n",
      "|  220|3250956842|         #NesQuik|\n",
      "|  220|3250956842|        #HongKong|\n",
      "|  220|3250956842|    #onlytwohours|\n",
      "|  220|5530025955|#PoorEdward!--But|\n",
      "|  220|5530025955|  #nosecondspring|\n",
      "+-----+----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdsrec\u001b[39m: \u001b[32mDataset\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = [utime: bigint, user_id: string ... 1 more field]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dsrec = values.select(from_json($\"value\", structure).alias(\"json\"))\n",
    "    .select($\"json.update_id\", $\"json.user_id\", $\"json.text\")\n",
    "    .select(callUDF(\"utime\", $\"update_id\").alias(\"utime\"),\n",
    "            $\"user_id\",\n",
    "            explode(split($\"text\", \" \")).alias(\"word\"))\n",
    "    .as[(Long, String, String)]\n",
    "    .filter(_._1 >= 220)\n",
    "    .filter(_._3(0)=='#')\n",
    "dsrec.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtka\u001b[39m: \u001b[32mTypedColumn\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)]] = topkaggregator()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tka = new org.isarnproject.sketches.udaf.TopKAggregator[(Long, String, String)](_._3).toColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdra\u001b[39m: \u001b[32mDataset\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)])] = [value: bigint, TopKAggregator(scala.Tuple3): binary]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dra = dsrec.groupByKey(_._1).agg(tka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres40\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)]] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    (\u001b[32m\"#first\"\u001b[39m, \u001b[32m31\u001b[39m),\n",
       "    (\u001b[32m\"#one\"\u001b[39m, \u001b[32m24\u001b[39m),\n",
       "    (\u001b[32m\"#Fivepounds\"\u001b[39m, \u001b[32m22\u001b[39m),\n",
       "    (\u001b[32m\"#thenextseason\"\u001b[39m, \u001b[32m22\u001b[39m),\n",
       "    (\u001b[32m\"#only2\"\u001b[39m, \u001b[32m19\u001b[39m)\n",
       "  ),\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    (\u001b[32m\"#first\"\u001b[39m, \u001b[32m32\u001b[39m),\n",
       "    (\u001b[32m\"#Elizabeth\"\u001b[39m, \u001b[32m22\u001b[39m),\n",
       "    (\u001b[32m\"#two\"\u001b[39m, \u001b[32m22\u001b[39m),\n",
       "    (\u001b[32m\"#MissTilney\"\u001b[39m, \u001b[32m21\u001b[39m),\n",
       "    (\u001b[32m\"#Jell-OSugar\"\u001b[39m, \u001b[32m20\u001b[39m)\n",
       "  ),\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    (\u001b[32m\"#first\"\u001b[39m, \u001b[32m26\u001b[39m),\n",
       "    (\u001b[32m\"#One\"\u001b[39m, \u001b[32m22\u001b[39m),\n",
       "    (\u001b[32m\"#Anne\"\u001b[39m, \u001b[32m22\u001b[39m),\n",
       "    (\u001b[32m\"#one\"\u001b[39m, \u001b[32m22\u001b[39m),\n",
       "    (\u001b[32m\"#Dingos\"\u001b[39m, \u001b[32m21\u001b[39m)\n",
       "  ),\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    (\u001b[32m\"#first\"\u001b[39m, \u001b[32m30\u001b[39m),\n",
       "    (\u001b[32m\"#second\"\u001b[39m, \u001b[32m23\u001b[39m),\n",
       "    (\u001b[32m\"#almost3days\"\u001b[39m, \u001b[32m21\u001b[39m),\n",
       "    (\u001b[32m\"#one\"\u001b[39m, \u001b[32m21\u001b[39m),\n",
       "    (\u001b[32m\"#Elizabeth\"\u001b[39m, \u001b[32m20\u001b[39m)\n",
       "  ),\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    (\u001b[32m\"#first\"\u001b[39m, \u001b[32m33\u001b[39m),\n",
       "    (\u001b[32m\"#two\"\u001b[39m, \u001b[32m24\u001b[39m),\n",
       "    (\u001b[32m\"#Elizabeth\"\u001b[39m, \u001b[32m23\u001b[39m),\n",
       "    (\u001b[32m\"#over$12\"\u001b[39m, \u001b[32m22\u001b[39m),\n",
       "    (\u001b[32m\"#SAM\"\u001b[39m, \u001b[32m20\u001b[39m)\n",
       "  ),\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    (\u001b[32m\"#first\"\u001b[39m, \u001b[32m31\u001b[39m),\n",
       "    (\u001b[32m\"#one\"\u001b[39m, \u001b[32m25\u001b[39m),\n",
       "..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dra.map { case (ut, tk) => tk }.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
