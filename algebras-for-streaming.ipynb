{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// put the spark install from the base notebook image onto Ammonite's classpath\n",
    "java.nio.file.Files.list(java.nio.file.Paths.get(\"/opt/spark/jars\")).toArray.map(_.toString).foreach { fname =>\n",
    "  val path = java.nio.file.FileSystems.getDefault().getPath(fname)\n",
    "  val x = ammonite.ops.Path(path)\n",
    "  interp.load.cp(x)\n",
    "}\n",
    "// Load the ammonite-spark package to get AmmoniteSparkSession\n",
    "import $ivy.`sh.almond::ammonite-spark:0.1.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql-kafka-0-10:2.2.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                \u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//import $ivy.`org.isarnproject::isarn-sketches-spark:0.3.1-sp2.2-py2.7`\n",
    "import $ivy.`org.isarnproject::isarn-sketches-spark:0.3.1-topk-1-sp2.2-py2.7`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting spark JARs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.eclipse.jetty.util.log).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "18/09/27 00:19:28 INFO SparkContext: Running Spark version 2.2.0\n",
      "18/09/27 00:19:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "18/09/27 00:19:29 INFO SparkContext: Submitted application: 025885b5-5ab0-44af-bfbb-dd159bb22f66\n",
      "18/09/27 00:19:29 INFO SecurityManager: Changing view acls to: 1000130000\n",
      "18/09/27 00:19:29 INFO SecurityManager: Changing modify acls to: 1000130000\n",
      "18/09/27 00:19:29 INFO SecurityManager: Changing view acls groups to: \n",
      "18/09/27 00:19:29 INFO SecurityManager: Changing modify acls groups to: \n",
      "18/09/27 00:19:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1000130000); groups with view permissions: Set(); users  with modify permissions: Set(1000130000); groups with modify permissions: Set()\n",
      "18/09/27 00:19:29 INFO Utils: Successfully started service 'sparkDriver' on port 36951.\n",
      "18/09/27 00:19:29 INFO SparkEnv: Registering MapOutputTracker\n",
      "18/09/27 00:19:29 INFO SparkEnv: Registering BlockManagerMaster\n",
      "18/09/27 00:19:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "18/09/27 00:19:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "18/09/27 00:19:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3ee9a996-d483-4204-9c02-c72992be1672\n",
      "18/09/27 00:19:29 INFO MemoryStore: MemoryStore started with capacity 1909.8 MB\n",
      "18/09/27 00:19:29 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "18/09/27 00:19:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "18/09/27 00:19:29 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.17.0.10:4040\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/upickle_2.11/0.6.6/upickle_2.11-0.6.6.jar at spark://172.17.0.10:36951/jars/upickle_2.11-0.6.6.jar with timestamp 1538007569981\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fastparse_2.11/1.0.0/fastparse_2.11-1.0.0.jar at spark://172.17.0.10:36951/jars/fastparse_2.11-1.0.0.jar with timestamp 1538007569984\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/jline/jline-terminal/3.6.2/jline-terminal-3.6.2.jar at spark://172.17.0.10:36951/jars/jline-terminal-3.6.2.jar with timestamp 1538007569984\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/scala-kernel-api_2.11.12/0.1.7/scala-kernel-api_2.11.12-0.1.7.jar at spark://172.17.0.10:36951/jars/scala-kernel-api_2.11.12-0.1.7.jar with timestamp 1538007569985\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/io/get-coursier/coursier_2.11/1.1.0-M7/coursier_2.11-1.1.0-M7.jar at spark://172.17.0.10:36951/jars/coursier_2.11-1.1.0-M7.jar with timestamp 1538007569985\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar at spark://172.17.0.10:36951/jars/javassist-3.21.0-GA.jar with timestamp 1538007569986\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scalaj/scalaj-http_2.11/2.4.0/scalaj-http_2.11-2.4.0.jar at spark://172.17.0.10:36951/jars/scalaj-http_2.11-2.4.0.jar with timestamp 1538007569986\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fansi_2.11/0.2.4/fansi_2.11-0.2.4.jar at spark://172.17.0.10:36951/jars/fansi_2.11-0.2.4.jar with timestamp 1538007569986\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.11/0.1.4/sourcecode_2.11-0.1.4.jar at spark://172.17.0.10:36951/jars/sourcecode_2.11-0.1.4.jar with timestamp 1538007569987\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/github/scopt/scopt_2.11/3.5.0/scopt_2.11-3.5.0.jar at spark://172.17.0.10:36951/jars/scopt_2.11-3.5.0.jar with timestamp 1538007569987\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/jline/jline-reader/3.6.2/jline-reader-3.6.2.jar at spark://172.17.0.10:36951/jars/jline-reader-3.6.2.jar with timestamp 1538007569988\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/net/java/dev/jna/jna/4.2.2/jna-4.2.2.jar at spark://172.17.0.10:36951/jars/jna-4.2.2.jar with timestamp 1538007569988\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-interp_2.11.12/1.1.2-37-53fcfcd/ammonite-interp_2.11.12-1.1.2-37-53fcfcd.jar at spark://172.17.0.10:36951/jars/ammonite-interp_2.11.12-1.1.2-37-53fcfcd.jar with timestamp 1538007569989\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-repl_2.11.12/1.1.2-37-53fcfcd/ammonite-repl_2.11.12-1.1.2-37-53fcfcd.jar at spark://172.17.0.10:36951/jars/ammonite-repl_2.11.12-1.1.2-37-53fcfcd.jar with timestamp 1538007569990\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-util_2.11/1.1.2-37-53fcfcd/ammonite-util_2.11-1.1.2-37-53fcfcd.jar at spark://172.17.0.10:36951/jars/ammonite-util_2.11-1.1.2-37-53fcfcd.jar with timestamp 1538007569990\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/pprint_2.11/0.5.2/pprint_2.11-0.5.2.jar at spark://172.17.0.10:36951/jars/pprint_2.11-0.5.2.jar with timestamp 1538007569990\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-ops_2.11/1.1.2-37-53fcfcd/ammonite-ops_2.11-1.1.2-37-53fcfcd.jar at spark://172.17.0.10:36951/jars/ammonite-ops_2.11-1.1.2-37-53fcfcd.jar with timestamp 1538007569991\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/io/get-coursier/coursier-cache_2.11/1.1.0-M7/coursier-cache_2.11-1.1.0-M7.jar at spark://172.17.0.10:36951/jars/coursier-cache_2.11-1.1.0-M7.jar with timestamp 1538007569991\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-runtime_2.11/1.1.2-37-53fcfcd/ammonite-runtime_2.11-1.1.2-37-53fcfcd.jar at spark://172.17.0.10:36951/jars/ammonite-runtime_2.11-1.1.2-37-53fcfcd.jar with timestamp 1538007569991\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/geny_2.11/0.1.2/geny_2.11-0.1.2.jar at spark://172.17.0.10:36951/jars/geny_2.11-0.1.2.jar with timestamp 1538007569992\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/scalaparse_2.11/1.0.0/scalaparse_2.11-1.0.0.jar at spark://172.17.0.10:36951/jars/scalaparse_2.11-1.0.0.jar with timestamp 1538007569992\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-terminal_2.11/1.1.2-37-53fcfcd/ammonite-terminal_2.11-1.1.2-37-53fcfcd.jar at spark://172.17.0.10:36951/jars/ammonite-terminal_2.11-1.1.2-37-53fcfcd.jar with timestamp 1538007569992\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/interpreter-api_2.11/0.1.7/interpreter-api_2.11-0.1.7.jar at spark://172.17.0.10:36951/jars/interpreter-api_2.11-0.1.7.jar with timestamp 1538007569992\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fastparse-utils_2.11/1.0.0/fastparse-utils_2.11-1.0.0.jar at spark://172.17.0.10:36951/jars/fastparse-utils_2.11-1.0.0.jar with timestamp 1538007569993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.1.0/scala-xml_2.11-1.1.0.jar at spark://172.17.0.10:36951/jars/scala-xml_2.11-1.1.0.jar with timestamp 1538007569993\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/jline/jline-terminal-jna/3.6.2/jline-terminal-jna-3.6.2.jar at spark://172.17.0.10:36951/jars/jline-terminal-jna-3.6.2.jar with timestamp 1538007569993\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ujson_2.11/0.6.6/ujson_2.11-0.6.6.jar at spark://172.17.0.10:36951/jars/ujson_2.11-0.6.6.jar with timestamp 1538007569994\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/com/github/javaparser/javaparser-core/3.2.5/javaparser-core-3.2.5.jar at spark://172.17.0.10:36951/jars/javaparser-core-3.2.5.jar with timestamp 1538007569994\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/home/nbuser/.local/share/jupyter/kernels/scala/launcher.jar at spark://172.17.0.10:36951/jars/launcher.jar with timestamp 1538007569994\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-continuation/8.1.14.v20131031/jetty-continuation-8.1.14.v20131031.jar at spark://172.17.0.10:36951/jars/jetty-continuation-8.1.14.v20131031.jar with timestamp 1538007569995\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-http/8.1.14.v20131031/jetty-http-8.1.14.v20131031.jar at spark://172.17.0.10:36951/jars/jetty-http-8.1.14.v20131031.jar with timestamp 1538007569995\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-server/8.1.14.v20131031/jetty-server-8.1.14.v20131031.jar at spark://172.17.0.10:36951/jars/jetty-server-8.1.14.v20131031.jar with timestamp 1538007569995\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/ammonite-spark_2.11/0.1.1/ammonite-spark_2.11-0.1.1.jar at spark://172.17.0.10:36951/jars/ammonite-spark_2.11-0.1.1.jar with timestamp 1538007569995\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-io/8.1.14.v20131031/jetty-io-8.1.14.v20131031.jar at spark://172.17.0.10:36951/jars/jetty-io-8.1.14.v20131031.jar with timestamp 1538007569996\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/8.1.14.v20131031/jetty-util-8.1.14.v20131031.jar at spark://172.17.0.10:36951/jars/jetty-util-8.1.14.v20131031.jar with timestamp 1538007569996\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/jars/JavaEWAH-0.3.2.jar at spark://172.17.0.10:36951/jars/JavaEWAH-0.3.2.jar with timestamp 1538007569996\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/jars/RoaringBitmap-0.5.11.jar at spark://172.17.0.10:36951/jars/RoaringBitmap-0.5.11.jar with timestamp 1538007569997\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/jars/ST4-4.0.4.jar at spark://172.17.0.10:36951/jars/ST4-4.0.4.jar with timestamp 1538007569997\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/jars/activation-1.1.1.jar at spark://172.17.0.10:36951/jars/activation-1.1.1.jar with timestamp 1538007569998\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/jars/antlr-2.7.7.jar at spark://172.17.0.10:36951/jars/antlr-2.7.7.jar with timestamp 1538007569999\n",
      "18/09/27 00:19:29 INFO SparkContext: Added JAR file:/opt/spark/jars/antlr-runtime-3.4.jar at spark://172.17.0.10:36951/jars/antlr-runtime-3.4.jar with timestamp 1538007569999\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/antlr4-runtime-4.5.3.jar at spark://172.17.0.10:36951/jars/antlr4-runtime-4.5.3.jar with timestamp 1538007570000\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/aopalliance-1.0.jar at spark://172.17.0.10:36951/jars/aopalliance-1.0.jar with timestamp 1538007570000\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/aopalliance-repackaged-2.4.0-b34.jar at spark://172.17.0.10:36951/jars/aopalliance-repackaged-2.4.0-b34.jar with timestamp 1538007570000\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/apache-log4j-extras-1.2.17.jar at spark://172.17.0.10:36951/jars/apache-log4j-extras-1.2.17.jar with timestamp 1538007570001\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/apacheds-i18n-2.0.0-M15.jar at spark://172.17.0.10:36951/jars/apacheds-i18n-2.0.0-M15.jar with timestamp 1538007570001\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/apacheds-kerberos-codec-2.0.0-M15.jar at spark://172.17.0.10:36951/jars/apacheds-kerberos-codec-2.0.0-M15.jar with timestamp 1538007570001\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/api-asn1-api-1.0.0-M20.jar at spark://172.17.0.10:36951/jars/api-asn1-api-1.0.0-M20.jar with timestamp 1538007570002\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/api-util-1.0.0-M20.jar at spark://172.17.0.10:36951/jars/api-util-1.0.0-M20.jar with timestamp 1538007570002\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/arpack_combined_all-0.1.jar at spark://172.17.0.10:36951/jars/arpack_combined_all-0.1.jar with timestamp 1538007570002\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/avro-1.7.7.jar at spark://172.17.0.10:36951/jars/avro-1.7.7.jar with timestamp 1538007570003\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/avro-ipc-1.7.7.jar at spark://172.17.0.10:36951/jars/avro-ipc-1.7.7.jar with timestamp 1538007570006\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/avro-mapred-1.7.7-hadoop2.jar at spark://172.17.0.10:36951/jars/avro-mapred-1.7.7-hadoop2.jar with timestamp 1538007570009\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/base64-2.3.8.jar at spark://172.17.0.10:36951/jars/base64-2.3.8.jar with timestamp 1538007570012\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/bcprov-jdk15on-1.51.jar at spark://172.17.0.10:36951/jars/bcprov-jdk15on-1.51.jar with timestamp 1538007570017\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/bonecp-0.8.0.RELEASE.jar at spark://172.17.0.10:36951/jars/bonecp-0.8.0.RELEASE.jar with timestamp 1538007570022\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/breeze-macros_2.11-0.13.1.jar at spark://172.17.0.10:36951/jars/breeze-macros_2.11-0.13.1.jar with timestamp 1538007570024\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/breeze_2.11-0.13.1.jar at spark://172.17.0.10:36951/jars/breeze_2.11-0.13.1.jar with timestamp 1538007570027\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/calcite-avatica-1.2.0-incubating.jar at spark://172.17.0.10:36951/jars/calcite-avatica-1.2.0-incubating.jar with timestamp 1538007570030\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/calcite-core-1.2.0-incubating.jar at spark://172.17.0.10:36951/jars/calcite-core-1.2.0-incubating.jar with timestamp 1538007570034\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/calcite-linq4j-1.2.0-incubating.jar at spark://172.17.0.10:36951/jars/calcite-linq4j-1.2.0-incubating.jar with timestamp 1538007570059\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/chill-java-0.8.0.jar at spark://172.17.0.10:36951/jars/chill-java-0.8.0.jar with timestamp 1538007570079\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/chill_2.11-0.8.0.jar at spark://172.17.0.10:36951/jars/chill_2.11-0.8.0.jar with timestamp 1538007570097\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-beanutils-1.7.0.jar at spark://172.17.0.10:36951/jars/commons-beanutils-1.7.0.jar with timestamp 1538007570117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-beanutils-core-1.8.0.jar at spark://172.17.0.10:36951/jars/commons-beanutils-core-1.8.0.jar with timestamp 1538007570136\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-cli-1.2.jar at spark://172.17.0.10:36951/jars/commons-cli-1.2.jar with timestamp 1538007570149\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-codec-1.10.jar at spark://172.17.0.10:36951/jars/commons-codec-1.10.jar with timestamp 1538007570161\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-collections-3.2.2.jar at spark://172.17.0.10:36951/jars/commons-collections-3.2.2.jar with timestamp 1538007570170\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-compiler-3.0.0.jar at spark://172.17.0.10:36951/jars/commons-compiler-3.0.0.jar with timestamp 1538007570174\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-compress-1.4.1.jar at spark://172.17.0.10:36951/jars/commons-compress-1.4.1.jar with timestamp 1538007570177\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-configuration-1.6.jar at spark://172.17.0.10:36951/jars/commons-configuration-1.6.jar with timestamp 1538007570180\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-crypto-1.0.0.jar at spark://172.17.0.10:36951/jars/commons-crypto-1.0.0.jar with timestamp 1538007570188\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-dbcp-1.4.jar at spark://172.17.0.10:36951/jars/commons-dbcp-1.4.jar with timestamp 1538007570190\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-digester-1.8.jar at spark://172.17.0.10:36951/jars/commons-digester-1.8.jar with timestamp 1538007570193\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-httpclient-3.1.jar at spark://172.17.0.10:36951/jars/commons-httpclient-3.1.jar with timestamp 1538007570194\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-io-2.4.jar at spark://172.17.0.10:36951/jars/commons-io-2.4.jar with timestamp 1538007570195\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-lang-2.6.jar at spark://172.17.0.10:36951/jars/commons-lang-2.6.jar with timestamp 1538007570197\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-lang3-3.5.jar at spark://172.17.0.10:36951/jars/commons-lang3-3.5.jar with timestamp 1538007570200\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-logging-1.1.3.jar at spark://172.17.0.10:36951/jars/commons-logging-1.1.3.jar with timestamp 1538007570202\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-math3-3.4.1.jar at spark://172.17.0.10:36951/jars/commons-math3-3.4.1.jar with timestamp 1538007570203\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-net-2.2.jar at spark://172.17.0.10:36951/jars/commons-net-2.2.jar with timestamp 1538007570204\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/commons-pool-1.5.4.jar at spark://172.17.0.10:36951/jars/commons-pool-1.5.4.jar with timestamp 1538007570205\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/compress-lzf-1.0.3.jar at spark://172.17.0.10:36951/jars/compress-lzf-1.0.3.jar with timestamp 1538007570207\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/core-1.1.2.jar at spark://172.17.0.10:36951/jars/core-1.1.2.jar with timestamp 1538007570208\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/curator-client-2.6.0.jar at spark://172.17.0.10:36951/jars/curator-client-2.6.0.jar with timestamp 1538007570210\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/curator-framework-2.6.0.jar at spark://172.17.0.10:36951/jars/curator-framework-2.6.0.jar with timestamp 1538007570211\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/curator-recipes-2.6.0.jar at spark://172.17.0.10:36951/jars/curator-recipes-2.6.0.jar with timestamp 1538007570212\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/datanucleus-api-jdo-3.2.6.jar at spark://172.17.0.10:36951/jars/datanucleus-api-jdo-3.2.6.jar with timestamp 1538007570215\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/datanucleus-core-3.2.10.jar at spark://172.17.0.10:36951/jars/datanucleus-core-3.2.10.jar with timestamp 1538007570227\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/datanucleus-rdbms-3.2.9.jar at spark://172.17.0.10:36951/jars/datanucleus-rdbms-3.2.9.jar with timestamp 1538007570227\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/derby-10.12.1.1.jar at spark://172.17.0.10:36951/jars/derby-10.12.1.1.jar with timestamp 1538007570230\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/eigenbase-properties-1.1.5.jar at spark://172.17.0.10:36951/jars/eigenbase-properties-1.1.5.jar with timestamp 1538007570230\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/gson-2.2.4.jar at spark://172.17.0.10:36951/jars/gson-2.2.4.jar with timestamp 1538007570232\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/guava-14.0.1.jar at spark://172.17.0.10:36951/jars/guava-14.0.1.jar with timestamp 1538007570236\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/guice-3.0.jar at spark://172.17.0.10:36951/jars/guice-3.0.jar with timestamp 1538007570236\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/guice-servlet-3.0.jar at spark://172.17.0.10:36951/jars/guice-servlet-3.0.jar with timestamp 1538007570238\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-annotations-2.7.3.jar at spark://172.17.0.10:36951/jars/hadoop-annotations-2.7.3.jar with timestamp 1538007570240\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-auth-2.7.3.jar at spark://172.17.0.10:36951/jars/hadoop-auth-2.7.3.jar with timestamp 1538007570242\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-client-2.7.3.jar at spark://172.17.0.10:36951/jars/hadoop-client-2.7.3.jar with timestamp 1538007570243\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-common-2.7.3.jar at spark://172.17.0.10:36951/jars/hadoop-common-2.7.3.jar with timestamp 1538007570249\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-hdfs-2.7.3.jar at spark://172.17.0.10:36951/jars/hadoop-hdfs-2.7.3.jar with timestamp 1538007570252\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-mapreduce-client-app-2.7.3.jar at spark://172.17.0.10:36951/jars/hadoop-mapreduce-client-app-2.7.3.jar with timestamp 1538007570253\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-mapreduce-client-common-2.7.3.jar at spark://172.17.0.10:36951/jars/hadoop-mapreduce-client-common-2.7.3.jar with timestamp 1538007570255\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-mapreduce-client-core-2.7.3.jar at spark://172.17.0.10:36951/jars/hadoop-mapreduce-client-core-2.7.3.jar with timestamp 1538007570256\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar at spark://172.17.0.10:36951/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar with timestamp 1538007570258\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar at spark://172.17.0.10:36951/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar with timestamp 1538007570260\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-yarn-api-2.7.3.jar at spark://172.17.0.10:36951/jars/hadoop-yarn-api-2.7.3.jar with timestamp 1538007570262\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-yarn-client-2.7.3.jar at spark://172.17.0.10:36951/jars/hadoop-yarn-client-2.7.3.jar with timestamp 1538007570263\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-yarn-common-2.7.3.jar at spark://172.17.0.10:36951/jars/hadoop-yarn-common-2.7.3.jar with timestamp 1538007570265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-yarn-server-common-2.7.3.jar at spark://172.17.0.10:36951/jars/hadoop-yarn-server-common-2.7.3.jar with timestamp 1538007570268\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hadoop-yarn-server-web-proxy-2.7.3.jar at spark://172.17.0.10:36951/jars/hadoop-yarn-server-web-proxy-2.7.3.jar with timestamp 1538007570269\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hive-beeline-1.2.1.spark2.jar at spark://172.17.0.10:36951/jars/hive-beeline-1.2.1.spark2.jar with timestamp 1538007570273\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hive-cli-1.2.1.spark2.jar at spark://172.17.0.10:36951/jars/hive-cli-1.2.1.spark2.jar with timestamp 1538007570274\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hive-exec-1.2.1.spark2.jar at spark://172.17.0.10:36951/jars/hive-exec-1.2.1.spark2.jar with timestamp 1538007570275\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hive-jdbc-1.2.1.spark2.jar at spark://172.17.0.10:36951/jars/hive-jdbc-1.2.1.spark2.jar with timestamp 1538007570276\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hive-metastore-1.2.1.spark2.jar at spark://172.17.0.10:36951/jars/hive-metastore-1.2.1.spark2.jar with timestamp 1538007570278\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hk2-api-2.4.0-b34.jar at spark://172.17.0.10:36951/jars/hk2-api-2.4.0-b34.jar with timestamp 1538007570280\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hk2-locator-2.4.0-b34.jar at spark://172.17.0.10:36951/jars/hk2-locator-2.4.0-b34.jar with timestamp 1538007570286\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/hk2-utils-2.4.0-b34.jar at spark://172.17.0.10:36951/jars/hk2-utils-2.4.0-b34.jar with timestamp 1538007570289\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/htrace-core-3.1.0-incubating.jar at spark://172.17.0.10:36951/jars/htrace-core-3.1.0-incubating.jar with timestamp 1538007570290\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/httpclient-4.5.2.jar at spark://172.17.0.10:36951/jars/httpclient-4.5.2.jar with timestamp 1538007570294\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/httpcore-4.4.4.jar at spark://172.17.0.10:36951/jars/httpcore-4.4.4.jar with timestamp 1538007570297\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/ivy-2.4.0.jar at spark://172.17.0.10:36951/jars/ivy-2.4.0.jar with timestamp 1538007570300\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jackson-annotations-2.6.5.jar at spark://172.17.0.10:36951/jars/jackson-annotations-2.6.5.jar with timestamp 1538007570303\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jackson-core-2.6.5.jar at spark://172.17.0.10:36951/jars/jackson-core-2.6.5.jar with timestamp 1538007570306\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jackson-core-asl-1.9.13.jar at spark://172.17.0.10:36951/jars/jackson-core-asl-1.9.13.jar with timestamp 1538007570309\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jackson-databind-2.6.5.jar at spark://172.17.0.10:36951/jars/jackson-databind-2.6.5.jar with timestamp 1538007570312\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jackson-jaxrs-1.9.13.jar at spark://172.17.0.10:36951/jars/jackson-jaxrs-1.9.13.jar with timestamp 1538007570318\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jackson-mapper-asl-1.9.13.jar at spark://172.17.0.10:36951/jars/jackson-mapper-asl-1.9.13.jar with timestamp 1538007570325\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jackson-module-paranamer-2.6.5.jar at spark://172.17.0.10:36951/jars/jackson-module-paranamer-2.6.5.jar with timestamp 1538007570341\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jackson-module-scala_2.11-2.6.5.jar at spark://172.17.0.10:36951/jars/jackson-module-scala_2.11-2.6.5.jar with timestamp 1538007570351\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jackson-xc-1.9.13.jar at spark://172.17.0.10:36951/jars/jackson-xc-1.9.13.jar with timestamp 1538007570355\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/janino-3.0.0.jar at spark://172.17.0.10:36951/jars/janino-3.0.0.jar with timestamp 1538007570359\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/java-xmlbuilder-1.0.jar at spark://172.17.0.10:36951/jars/java-xmlbuilder-1.0.jar with timestamp 1538007570361\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/javassist-3.18.1-GA.jar at spark://172.17.0.10:36951/jars/javassist-3.18.1-GA.jar with timestamp 1538007570365\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/javax.annotation-api-1.2.jar at spark://172.17.0.10:36951/jars/javax.annotation-api-1.2.jar with timestamp 1538007570368\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/javax.inject-1.jar at spark://172.17.0.10:36951/jars/javax.inject-1.jar with timestamp 1538007570371\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/javax.inject-2.4.0-b34.jar at spark://172.17.0.10:36951/jars/javax.inject-2.4.0-b34.jar with timestamp 1538007570373\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/javax.servlet-api-3.1.0.jar at spark://172.17.0.10:36951/jars/javax.servlet-api-3.1.0.jar with timestamp 1538007570376\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/javax.ws.rs-api-2.0.1.jar at spark://172.17.0.10:36951/jars/javax.ws.rs-api-2.0.1.jar with timestamp 1538007570378\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/javolution-5.5.1.jar at spark://172.17.0.10:36951/jars/javolution-5.5.1.jar with timestamp 1538007570380\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jaxb-api-2.2.2.jar at spark://172.17.0.10:36951/jars/jaxb-api-2.2.2.jar with timestamp 1538007570385\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jcl-over-slf4j-1.7.16.jar at spark://172.17.0.10:36951/jars/jcl-over-slf4j-1.7.16.jar with timestamp 1538007570387\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jdo-api-3.0.1.jar at spark://172.17.0.10:36951/jars/jdo-api-3.0.1.jar with timestamp 1538007570390\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jersey-client-2.22.2.jar at spark://172.17.0.10:36951/jars/jersey-client-2.22.2.jar with timestamp 1538007570392\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jersey-common-2.22.2.jar at spark://172.17.0.10:36951/jars/jersey-common-2.22.2.jar with timestamp 1538007570394\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jersey-container-servlet-2.22.2.jar at spark://172.17.0.10:36951/jars/jersey-container-servlet-2.22.2.jar with timestamp 1538007570397\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jersey-container-servlet-core-2.22.2.jar at spark://172.17.0.10:36951/jars/jersey-container-servlet-core-2.22.2.jar with timestamp 1538007570403\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jersey-guava-2.22.2.jar at spark://172.17.0.10:36951/jars/jersey-guava-2.22.2.jar with timestamp 1538007570405\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jersey-media-jaxb-2.22.2.jar at spark://172.17.0.10:36951/jars/jersey-media-jaxb-2.22.2.jar with timestamp 1538007570408\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jersey-server-2.22.2.jar at spark://172.17.0.10:36951/jars/jersey-server-2.22.2.jar with timestamp 1538007570411\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jets3t-0.9.3.jar at spark://172.17.0.10:36951/jars/jets3t-0.9.3.jar with timestamp 1538007570420\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jetty-6.1.26.jar at spark://172.17.0.10:36951/jars/jetty-6.1.26.jar with timestamp 1538007570421\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jetty-util-6.1.26.jar at spark://172.17.0.10:36951/jars/jetty-util-6.1.26.jar with timestamp 1538007570422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jline-2.12.1.jar at spark://172.17.0.10:36951/jars/jline-2.12.1.jar with timestamp 1538007570424\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/joda-time-2.9.3.jar at spark://172.17.0.10:36951/jars/joda-time-2.9.3.jar with timestamp 1538007570425\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jodd-core-3.5.2.jar at spark://172.17.0.10:36951/jars/jodd-core-3.5.2.jar with timestamp 1538007570426\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jpam-1.1.jar at spark://172.17.0.10:36951/jars/jpam-1.1.jar with timestamp 1538007570427\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/json4s-ast_2.11-3.2.11.jar at spark://172.17.0.10:36951/jars/json4s-ast_2.11-3.2.11.jar with timestamp 1538007570429\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/json4s-core_2.11-3.2.11.jar at spark://172.17.0.10:36951/jars/json4s-core_2.11-3.2.11.jar with timestamp 1538007570431\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/json4s-jackson_2.11-3.2.11.jar at spark://172.17.0.10:36951/jars/json4s-jackson_2.11-3.2.11.jar with timestamp 1538007570435\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jsp-api-2.1.jar at spark://172.17.0.10:36951/jars/jsp-api-2.1.jar with timestamp 1538007570436\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jsr305-1.3.9.jar at spark://172.17.0.10:36951/jars/jsr305-1.3.9.jar with timestamp 1538007570438\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jta-1.1.jar at spark://172.17.0.10:36951/jars/jta-1.1.jar with timestamp 1538007570440\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jtransforms-2.4.0.jar at spark://172.17.0.10:36951/jars/jtransforms-2.4.0.jar with timestamp 1538007570442\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/jul-to-slf4j-1.7.16.jar at spark://172.17.0.10:36951/jars/jul-to-slf4j-1.7.16.jar with timestamp 1538007570444\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/kryo-shaded-3.0.3.jar at spark://172.17.0.10:36951/jars/kryo-shaded-3.0.3.jar with timestamp 1538007570446\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/leveldbjni-all-1.8.jar at spark://172.17.0.10:36951/jars/leveldbjni-all-1.8.jar with timestamp 1538007570450\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/libfb303-0.9.3.jar at spark://172.17.0.10:36951/jars/libfb303-0.9.3.jar with timestamp 1538007570452\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/libthrift-0.9.3.jar at spark://172.17.0.10:36951/jars/libthrift-0.9.3.jar with timestamp 1538007570453\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/log4j-1.2.17.jar at spark://172.17.0.10:36951/jars/log4j-1.2.17.jar with timestamp 1538007570455\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/lz4-1.3.0.jar at spark://172.17.0.10:36951/jars/lz4-1.3.0.jar with timestamp 1538007570456\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/machinist_2.11-0.6.1.jar at spark://172.17.0.10:36951/jars/machinist_2.11-0.6.1.jar with timestamp 1538007570457\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/macro-compat_2.11-1.1.1.jar at spark://172.17.0.10:36951/jars/macro-compat_2.11-1.1.1.jar with timestamp 1538007570458\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/mail-1.4.7.jar at spark://172.17.0.10:36951/jars/mail-1.4.7.jar with timestamp 1538007570459\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/mesos-1.0.0-shaded-protobuf.jar at spark://172.17.0.10:36951/jars/mesos-1.0.0-shaded-protobuf.jar with timestamp 1538007570460\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/metrics-core-3.1.2.jar at spark://172.17.0.10:36951/jars/metrics-core-3.1.2.jar with timestamp 1538007570461\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/metrics-graphite-3.1.2.jar at spark://172.17.0.10:36951/jars/metrics-graphite-3.1.2.jar with timestamp 1538007570463\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/metrics-json-3.1.2.jar at spark://172.17.0.10:36951/jars/metrics-json-3.1.2.jar with timestamp 1538007570464\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/metrics-jvm-3.1.2.jar at spark://172.17.0.10:36951/jars/metrics-jvm-3.1.2.jar with timestamp 1538007570466\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/minlog-1.3.0.jar at spark://172.17.0.10:36951/jars/minlog-1.3.0.jar with timestamp 1538007570469\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/mx4j-3.0.2.jar at spark://172.17.0.10:36951/jars/mx4j-3.0.2.jar with timestamp 1538007570472\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/netty-3.9.9.Final.jar at spark://172.17.0.10:36951/jars/netty-3.9.9.Final.jar with timestamp 1538007570474\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/netty-all-4.0.43.Final.jar at spark://172.17.0.10:36951/jars/netty-all-4.0.43.Final.jar with timestamp 1538007570474\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/objenesis-2.1.jar at spark://172.17.0.10:36951/jars/objenesis-2.1.jar with timestamp 1538007570476\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/opencsv-2.3.jar at spark://172.17.0.10:36951/jars/opencsv-2.3.jar with timestamp 1538007570477\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/oro-2.0.8.jar at spark://172.17.0.10:36951/jars/oro-2.0.8.jar with timestamp 1538007570479\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/osgi-resource-locator-1.0.1.jar at spark://172.17.0.10:36951/jars/osgi-resource-locator-1.0.1.jar with timestamp 1538007570480\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/paranamer-2.6.jar at spark://172.17.0.10:36951/jars/paranamer-2.6.jar with timestamp 1538007570484\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/parquet-column-1.8.2.jar at spark://172.17.0.10:36951/jars/parquet-column-1.8.2.jar with timestamp 1538007570485\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/parquet-common-1.8.2.jar at spark://172.17.0.10:36951/jars/parquet-common-1.8.2.jar with timestamp 1538007570487\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/parquet-encoding-1.8.2.jar at spark://172.17.0.10:36951/jars/parquet-encoding-1.8.2.jar with timestamp 1538007570490\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/parquet-format-2.3.1.jar at spark://172.17.0.10:36951/jars/parquet-format-2.3.1.jar with timestamp 1538007570492\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/parquet-hadoop-1.8.2.jar at spark://172.17.0.10:36951/jars/parquet-hadoop-1.8.2.jar with timestamp 1538007570493\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/parquet-hadoop-bundle-1.6.0.jar at spark://172.17.0.10:36951/jars/parquet-hadoop-bundle-1.6.0.jar with timestamp 1538007570494\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/parquet-jackson-1.8.2.jar at spark://172.17.0.10:36951/jars/parquet-jackson-1.8.2.jar with timestamp 1538007570496\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/pmml-model-1.2.15.jar at spark://172.17.0.10:36951/jars/pmml-model-1.2.15.jar with timestamp 1538007570498\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/pmml-schema-1.2.15.jar at spark://172.17.0.10:36951/jars/pmml-schema-1.2.15.jar with timestamp 1538007570509\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/protobuf-java-2.5.0.jar at spark://172.17.0.10:36951/jars/protobuf-java-2.5.0.jar with timestamp 1538007570510\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/py4j-0.10.4.jar at spark://172.17.0.10:36951/jars/py4j-0.10.4.jar with timestamp 1538007570522\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/pyrolite-4.13.jar at spark://172.17.0.10:36951/jars/pyrolite-4.13.jar with timestamp 1538007570524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/scala-compiler-2.11.8.jar at spark://172.17.0.10:36951/jars/scala-compiler-2.11.8.jar with timestamp 1538007570525\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/scala-library-2.11.8.jar at spark://172.17.0.10:36951/jars/scala-library-2.11.8.jar with timestamp 1538007570527\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/scala-parser-combinators_2.11-1.0.4.jar at spark://172.17.0.10:36951/jars/scala-parser-combinators_2.11-1.0.4.jar with timestamp 1538007570530\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/scala-reflect-2.11.8.jar at spark://172.17.0.10:36951/jars/scala-reflect-2.11.8.jar with timestamp 1538007570537\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/scala-xml_2.11-1.0.2.jar at spark://172.17.0.10:36951/jars/scala-xml_2.11-1.0.2.jar with timestamp 1538007570539\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/scalap-2.11.8.jar at spark://172.17.0.10:36951/jars/scalap-2.11.8.jar with timestamp 1538007570541\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/shapeless_2.11-2.3.2.jar at spark://172.17.0.10:36951/jars/shapeless_2.11-2.3.2.jar with timestamp 1538007570544\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/slf4j-api-1.7.16.jar at spark://172.17.0.10:36951/jars/slf4j-api-1.7.16.jar with timestamp 1538007570546\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/slf4j-log4j12-1.7.16.jar at spark://172.17.0.10:36951/jars/slf4j-log4j12-1.7.16.jar with timestamp 1538007570551\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/snappy-0.2.jar at spark://172.17.0.10:36951/jars/snappy-0.2.jar with timestamp 1538007570555\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/snappy-java-1.1.2.6.jar at spark://172.17.0.10:36951/jars/snappy-java-1.1.2.6.jar with timestamp 1538007570557\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-catalyst_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-catalyst_2.11-2.2.0.jar with timestamp 1538007570564\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-core_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-core_2.11-2.2.0.jar with timestamp 1538007570587\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-graphx_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-graphx_2.11-2.2.0.jar with timestamp 1538007570603\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-hive-thriftserver_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-hive-thriftserver_2.11-2.2.0.jar with timestamp 1538007570607\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-hive_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-hive_2.11-2.2.0.jar with timestamp 1538007570637\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-launcher_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-launcher_2.11-2.2.0.jar with timestamp 1538007570644\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-mesos_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-mesos_2.11-2.2.0.jar with timestamp 1538007570649\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-mllib-local_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-mllib-local_2.11-2.2.0.jar with timestamp 1538007570656\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-mllib_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-mllib_2.11-2.2.0.jar with timestamp 1538007570660\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-network-common_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-network-common_2.11-2.2.0.jar with timestamp 1538007570664\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-network-shuffle_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-network-shuffle_2.11-2.2.0.jar with timestamp 1538007570673\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-repl_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-repl_2.11-2.2.0.jar with timestamp 1538007570685\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-sketch_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-sketch_2.11-2.2.0.jar with timestamp 1538007570689\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-sql_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-sql_2.11-2.2.0.jar with timestamp 1538007570694\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-streaming_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-streaming_2.11-2.2.0.jar with timestamp 1538007570701\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-tags_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-tags_2.11-2.2.0.jar with timestamp 1538007570709\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-unsafe_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-unsafe_2.11-2.2.0.jar with timestamp 1538007570715\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-yarn_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-yarn_2.11-2.2.0.jar with timestamp 1538007570722\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spire-macros_2.11-0.13.0.jar at spark://172.17.0.10:36951/jars/spire-macros_2.11-0.13.0.jar with timestamp 1538007570725\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/spire_2.11-0.13.0.jar at spark://172.17.0.10:36951/jars/spire_2.11-0.13.0.jar with timestamp 1538007570726\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/stax-api-1.0-2.jar at spark://172.17.0.10:36951/jars/stax-api-1.0-2.jar with timestamp 1538007570727\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/stax-api-1.0.1.jar at spark://172.17.0.10:36951/jars/stax-api-1.0.1.jar with timestamp 1538007570728\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/stream-2.7.0.jar at spark://172.17.0.10:36951/jars/stream-2.7.0.jar with timestamp 1538007570729\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/stringtemplate-3.2.1.jar at spark://172.17.0.10:36951/jars/stringtemplate-3.2.1.jar with timestamp 1538007570731\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/super-csv-2.2.0.jar at spark://172.17.0.10:36951/jars/super-csv-2.2.0.jar with timestamp 1538007570732\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/univocity-parsers-2.2.1.jar at spark://172.17.0.10:36951/jars/univocity-parsers-2.2.1.jar with timestamp 1538007570741\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/validation-api-1.1.0.Final.jar at spark://172.17.0.10:36951/jars/validation-api-1.1.0.Final.jar with timestamp 1538007570743\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/xbean-asm5-shaded-4.4.jar at spark://172.17.0.10:36951/jars/xbean-asm5-shaded-4.4.jar with timestamp 1538007570744\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/xercesImpl-2.9.1.jar at spark://172.17.0.10:36951/jars/xercesImpl-2.9.1.jar with timestamp 1538007570746\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/xmlenc-0.52.jar at spark://172.17.0.10:36951/jars/xmlenc-0.52.jar with timestamp 1538007570748\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/xz-1.0.jar at spark://172.17.0.10:36951/jars/xz-1.0.jar with timestamp 1538007570750\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/jars/zookeeper-3.4.6.jar at spark://172.17.0.10:36951/jars/zookeeper-3.4.6.jar with timestamp 1538007570751\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/kafka/kafka-clients/0.10.0.1/kafka-clients-0.10.0.1.jar at spark://172.17.0.10:36951/jars/kafka-clients-0.10.0.1.jar with timestamp 1538007570752\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.11/2.2.0/spark-sql-kafka-0-10_2.11-2.2.0.jar at spark://172.17.0.10:36951/jars/spark-sql-kafka-0-10_2.11-2.2.0.jar with timestamp 1538007570754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.21/slf4j-api-1.7.21.jar at spark://172.17.0.10:36951/jars/slf4j-api-1.7.21.jar with timestamp 1538007570754\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/isarnproject/isarn-sketches_2.11/0.1.2/isarn-sketches_2.11-0.1.2.jar at spark://172.17.0.10:36951/jars/isarn-sketches_2.11-0.1.2.jar with timestamp 1538007570755\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/isarnproject/isarn-algebra-api_2.11/0.0.3/isarn-algebra-api_2.11-0.0.3.jar at spark://172.17.0.10:36951/jars/isarn-algebra-api_2.11-0.0.3.jar with timestamp 1538007570756\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/isarnproject/isarn-sketches-spark_2.11/0.3.1-topk-1-sp2.2-py2.7/isarn-sketches-spark_2.11-0.3.1-topk-1-sp2.2-py2.7.jar at spark://172.17.0.10:36951/jars/isarn-sketches-spark_2.11-0.3.1-topk-1-sp2.2-py2.7.jar with timestamp 1538007570757\n",
      "18/09/27 00:19:30 INFO SparkContext: Added JAR file:/opt/spark/.cache/coursier/v1/https/repo1.maven.org/maven2/org/isarnproject/isarn-collections_2.11/0.0.4/isarn-collections_2.11-0.0.4.jar at spark://172.17.0.10:36951/jars/isarn-collections_2.11-0.0.4.jar with timestamp 1538007570758\n",
      "18/09/27 00:19:30 INFO Executor: Starting executor ID driver on host localhost\n",
      "18/09/27 00:19:30 INFO Executor: Using REPL class URI: http://172.17.0.10:34365\n",
      "18/09/27 00:19:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41195.\n",
      "18/09/27 00:19:30 INFO NettyBlockTransferService: Server created on 172.17.0.10:41195\n",
      "18/09/27 00:19:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "18/09/27 00:19:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.17.0.10, 41195, None)\n",
      "18/09/27 00:19:30 INFO BlockManagerMasterEndpoint: Registering block manager 172.17.0.10:41195 with 1909.8 MB RAM, BlockManagerId(driver, 172.17.0.10, 41195, None)\n",
      "18/09/27 00:19:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.17.0.10, 41195, None)\n",
      "18/09/27 00:19:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.17.0.10, 41195, None)\n",
      "18/09/27 00:19:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/notebooks/spark-warehouse/').\n",
      "18/09/27 00:19:31 INFO SharedState: Warehouse path is 'file:/notebooks/spark-warehouse/'.\n",
      "18/09/27 00:19:32 WARN SharedState: URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory\n",
      "18/09/27 00:19:32 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@5fe7b4fe"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "val spark = {\n",
    "    AmmoniteSparkSession.builder()\n",
    "      .master(\"local[2]\")\n",
    "      .getOrCreate()\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.sqlContext.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions._\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.sqlContext.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.expressions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mappender\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mlog4j\u001b[39m.\u001b[32mConsoleAppender\u001b[39m = org.apache.log4j.ConsoleAppender@2d347126"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val appender = org.apache.log4j.Logger.getRootLogger().getAppender(\"console\").asInstanceOf[org.apache.log4j.ConsoleAppender]\n",
    "appender.setThreshold(org.apache.log4j.Level.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres6_0\u001b[39m: \u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(\n",
       "  <function1>,\n",
       "  LongType,\n",
       "  \u001b[33mSome\u001b[39m(\u001b[33mList\u001b[39m(StringType))\n",
       ")\n",
       "\u001b[36mres6_1\u001b[39m: \u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(\n",
       "  <function1>,\n",
       "  IntegerType,\n",
       "  \u001b[33mSome\u001b[39m(\u001b[33mList\u001b[39m(StringType))\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"utime\", (x:String)=>x.toLong / 1000)\n",
    "spark.udf.register(\"wordcount\", (text: String)=>text.split(\" \").filter(_.length > 0).length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.isarnproject.sketches._, org.isarnproject.sketches.udaf._, org.apache.spark.isarnproject.sketches.udt._\n",
       "\u001b[39m\n",
       "\u001b[36msketchCDF\u001b[39m: \u001b[32mTDigestUDAF\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mTDigestUDAF\u001b[39m(\u001b[32m0.2\u001b[39m, \u001b[32m25\u001b[39m)\n",
       "\u001b[36mres7_2\u001b[39m: \u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(<function1>, DoubleType, \u001b[32mNone\u001b[39m)\n",
       "\u001b[36mres7_3\u001b[39m: \u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(<function1>, DoubleType, \u001b[32mNone\u001b[39m)\n",
       "\u001b[36mres7_4\u001b[39m: \u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(<function1>, DoubleType, \u001b[32mNone\u001b[39m)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.isarnproject.sketches._, org.isarnproject.sketches.udaf._, org.apache.spark.isarnproject.sketches.udt._\n",
    "val sketchCDF = tdigestUDAF[Double].delta(0.2).maxDiscrete(25)\n",
    "spark.udf.register(\"p50\", (c:Any)=>c.asInstanceOf[TDigestSQL].tdigest.cdfInverse(0.5))\n",
    "spark.udf.register(\"p90\", (c:Any)=>c.asInstanceOf[TDigestSQL].tdigest.cdfInverse(0.9))\n",
    "spark.udf.register(\"p99\", (c:Any)=>c.asInstanceOf[TDigestSQL].tdigest.cdfInverse(0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [key: binary, value: binary ... 5 more fields]\n",
       "\u001b[36mvalues\u001b[39m: \u001b[32mDataFrame\u001b[39m = [value: string]\n",
       "\u001b[36mstructure\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"text\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"user_id\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"update_id\"\u001b[39m, StringType, true, {})\n",
       ")\n",
       "\u001b[36mrecords\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [user_id: string, utime: bigint ... 1 more field]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark\n",
    "  .read\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "  .option(\"subscribe\", \"social-firehose\")\n",
    "  .load()\n",
    "val values = df.select(($\"value\").cast(StringType))\n",
    "val structure = StructType(Seq(\"text\",\"user_id\",\"update_id\").map{f=>StructField(f, StringType, true)})\n",
    "val records = values.select(from_json($\"value\", structure).alias(\"json\"))\n",
    "    .select($\"json.update_id\", $\"json.user_id\", $\"json.text\")\n",
    "    .select($\"user_id\",\n",
    "            callUDF(\"utime\", $\"update_id\").alias(\"utime\"),\n",
    "            callUDF(\"wordcount\", $\"text\").alias(\"wordcount\"))\n",
    "    .filter($\"utime\" > 220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/*\n",
    "val times = records.withColumn(\"time\",current_timestamp())\n",
    "val w = times.groupBy(window($\"time\",\"1 second\"), $\"user_id\").count()\n",
    "val query = w.writeStream\n",
    "  .outputMode(\"complete\")\n",
    "  .format(\"console\")\n",
    "  .start()\n",
    "query.awaitTermination\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/*\n",
    "val wspec = Window.partitionBy(\"utime\").rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "val w = records.withColumn(\"max\", max($\"user_id\").over(wspec))\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|   user_id| avg|\n",
      "+----------+----+\n",
      "|9438801796|42.0|\n",
      "|0837938601|41.0|\n",
      "|0004926696|40.0|\n",
      "|7439949213|39.0|\n",
      "|2505585758|39.0|\n",
      "+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mt\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [user_id: string, avg: double]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t = records.groupBy($\"user_id\")\n",
    "    .agg(avg($\"wordcount\").alias(\"avg\"))\n",
    "    .orderBy($\"avg\".desc)\n",
    "t.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+\n",
      "|utime|               p50|               p90|\n",
      "+-----+------------------+------------------+\n",
      "|  230|              15.0|              31.0|\n",
      "|  229|15.767441860465116|              31.0|\n",
      "|  228|15.808988764044944|30.031992563804113|\n",
      "|  227|              15.0|              30.0|\n",
      "|  226|              16.0|31.885714285714286|\n",
      "|  225|              15.0|29.949291631157948|\n",
      "|  224|16.817073170731707|              32.0|\n",
      "|  223|15.596012010611316|              31.0|\n",
      "|  222|15.605821947611172|              31.0|\n",
      "|  221| 15.56338028169014|              30.0|\n",
      "+-----+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mt\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [utime: bigint, CDF: tdigest]\n",
       "\u001b[36mt2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [utime: bigint, p50: double ... 1 more field]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t = records.groupBy($\"utime\")\n",
    "    .agg(sketchCDF($\"wordcount\").alias(\"CDF\"))\n",
    "    .orderBy($\"utime\".desc)\n",
    "val t2 = t.select(\n",
    "    $\"utime\",\n",
    "    callUDF(\"p50\", $\"CDF\").alias(\"p50\"),\n",
    "    callUDF(\"p90\", $\"CDF\").alias(\"p90\")\n",
    ")\n",
    "t2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------------+\n",
      "|utime|   user_id|             word|\n",
      "+-----+----------+-----------------+\n",
      "|  220|3250956842|         #NesQuik|\n",
      "|  220|3250956842|        #HongKong|\n",
      "|  220|3250956842|    #onlytwohours|\n",
      "|  220|5530025955|#PoorEdward!--But|\n",
      "|  220|5530025955|  #nosecondspring|\n",
      "+-----+----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdsrec\u001b[39m: \u001b[32mDataset\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = [utime: bigint, user_id: string ... 1 more field]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dsrec = values.select(from_json($\"value\", structure).alias(\"json\"))\n",
    "    .select($\"json.update_id\", $\"json.user_id\", $\"json.text\")\n",
    "    .select(callUDF(\"utime\", $\"update_id\").alias(\"utime\"),\n",
    "            $\"user_id\",\n",
    "            explode(split($\"text\", \" \")).alias(\"word\"))\n",
    "    .as[(Long, String, String)]\n",
    "    .filter(_._1 >= 220)\n",
    "    .filter(_._3(0)=='#')\n",
    "dsrec.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtka\u001b[39m: \u001b[32mTypedColumn\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)]] = topkaggregator()\n",
       "\u001b[36mdra\u001b[39m: \u001b[32mDataset\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)])] = [value: bigint, TopKAggregator(scala.Tuple3): binary]\n",
       "\u001b[36mres12_2\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)]] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    (\u001b[32m\"#first\"\u001b[39m, \u001b[32m31\u001b[39m),\n",
       "    (\u001b[32m\"#one\"\u001b[39m, \u001b[32m24\u001b[39m),\n",
       "    (\u001b[32m\"#Fivepounds\"\u001b[39m, \u001b[32m22\u001b[39m),\n",
       "    (\u001b[32m\"#thenextseason\"\u001b[39m, \u001b[32m22\u001b[39m),\n",
       "    (\u001b[32m\"#only2\"\u001b[39m, \u001b[32m19\u001b[39m)\n",
       "  ),\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    (\u001b[32m\"#first\"\u001b[39m, \u001b[32m32\u001b[39m),\n",
       "    (\u001b[32m\"#Elizabeth\"\u001b[39m, \u001b[32m22\u001b[39m),\n",
       "    (\u001b[32m\"#two\"\u001b[39m, \u001b[32m22\u001b[39m),\n",
       "    (\u001b[32m\"#MissTilney\"\u001b[39m, \u001b[32m21\u001b[39m),\n",
       "    (\u001b[32m\"#Jell-OSugar\"\u001b[39m, \u001b[32m20\u001b[39m)\n",
       "  ),\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    (\u001b[32m\"#first\"\u001b[39m, \u001b[32m26\u001b[39m),\n",
       "    (\u001b[32m\"#One\"\u001b[39m, \u001b[32m22\u001b[39m),\n",
       "    (\u001b[32m\"#Anne\"\u001b[39m, \u001b[32m22\u001b[39m),\n",
       "    (\u001b[32m\"#one\"\u001b[39m, \u001b[32m22\u001b[39m),\n",
       "    (\u001b[32m\"#Dingos\"\u001b[39m, \u001b[32m21\u001b[39m)\n",
       "  ),\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    (\u001b[32m\"#first\"\u001b[39m, \u001b[32m30\u001b[39m),\n",
       "    (\u001b[32m\"#second\"\u001b[39m, \u001b[32m23\u001b[39m),\n",
       "    (\u001b[32m\"#almost3days\"\u001b[39m, \u001b[32m21\u001b[39m),\n",
       "    (\u001b[32m\"#one\"\u001b[39m, \u001b[32m21\u001b[39m),\n",
       "    (\u001b[32m\"#Elizabeth\"\u001b[39m, \u001b[32m20\u001b[39m)\n",
       "  ),\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    (\u001b[32m\"#first\"\u001b[39m, \u001b[32m33\u001b[39m),\n",
       "    (\u001b[32m\"#two\"\u001b[39m, \u001b[32m24\u001b[39m),\n",
       "    (\u001b[32m\"#Elizabeth\"\u001b[39m, \u001b[32m23\u001b[39m),\n",
       "    (\u001b[32m\"#over$12\"\u001b[39m, \u001b[32m22\u001b[39m),\n",
       "    (\u001b[32m\"#SAM\"\u001b[39m, \u001b[32m20\u001b[39m)\n",
       "  ),\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    (\u001b[32m\"#first\"\u001b[39m, \u001b[32m31\u001b[39m),\n",
       "    (\u001b[32m\"#one\"\u001b[39m, \u001b[32m25\u001b[39m),\n",
       "..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tka = new org.isarnproject.sketches.udaf.TopKAggregator[(Long, String, String)](_._3).toColumn\n",
    "val dra = dsrec.groupByKey(_._1).agg(tka)\n",
    "dra.map { case (ut, tk) => tk }.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
