{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran this notebook in Openshift, in a container image that has Spark pre-installed. Ammonite can download spark packages but I wanted to avoid duplicating the jar files, so I added this code to place the preinstalled Spark jars on the kernel's path. If you want or need to download Spark using Ammonite `$ivy` magic, you can comment this cell out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "// put the spark install from the base notebook image onto Ammonite's classpath\n",
    "java.nio.file.Files.list(java.nio.file.Paths.get(\"/opt/spark/jars\")).toArray.map(_.toString).foreach { fname =>\n",
    "  val path = java.nio.file.FileSystems.getDefault().getPath(fname)\n",
    "  val x = ammonite.ops.Path(path)\n",
    "  interp.load.cp(x)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library support for Spark in Ammonite and Kafka streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \n",
       "// Load Spark's kafka streaming package\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load the ammonite-spark package to get AmmoniteSparkSession\n",
    "import $ivy.`sh.almond::ammonite-spark:0.1.1`\n",
    "// Load Spark's kafka streaming package\n",
    "import $ivy.`org.apache.spark::spark-sql-kafka-0-10:2.2.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This library provides UDAFs for sketching data distributions with T-Digests. I am loading a custom library rev that has my prototype aggregator for top-k (aka heavy-hitters, aka most-frequent-items):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                \u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//import $ivy.`org.isarnproject::isarn-sketches-spark:0.3.1-sp2.2-py2.7`\n",
    "import $ivy.`org.isarnproject::isarn-sketches-spark:0.3.1-topk-1-sp2.2-py2.7`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kick off a spark session using Ammonite's custom shim library. I'm just running a couple workers directly in the container but you can also connect to an external cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "val spark = {\n",
    "    AmmoniteSparkSession.builder()\n",
    "      .master(\"local[2]\")\n",
    "      .getOrCreate()\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of my imports in one cell.\n",
    "Mostly Spark DataFrame & Structured Streaming definitions, plus some sketching definitions\n",
    "from `isarn-sketches-spark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.sqlContext.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.streaming.Trigger\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.isarnproject.sketches._, org.isarnproject.sketches.udaf._, org.apache.spark.isarnproject.sketches.udt._\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.sqlContext.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.isarnproject.sketches._, org.isarnproject.sketches.udaf._, org.apache.spark.isarnproject.sketches.udt._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bit of code just sets spark's logging so that only warnings or errors show up in the output cells,\n",
    "which makes notebook output much more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mappender\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mlog4j\u001b[39m.\u001b[32mConsoleAppender\u001b[39m = org.apache.log4j.ConsoleAppender@23b1156"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val appender = org.apache.log4j.Logger.getRootLogger().getAppender(\"console\").asInstanceOf[org.apache.log4j.ConsoleAppender]\n",
    "appender.setThreshold(org.apache.log4j.Level.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines some UDFs and UDAF:\n",
    "\n",
    "* A UDF to generate a wordcount from a field of text\n",
    "* A UDAF to sketch the CDF of some numeric data using a t-digest\n",
    "* Some UDFs that extract percentiles from a CDF sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres6_0\u001b[39m: \u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(\n",
       "  <function1>,\n",
       "  IntegerType,\n",
       "  \u001b[33mSome\u001b[39m(\u001b[33mList\u001b[39m(StringType))\n",
       ")\n",
       "\u001b[36msketchCDF\u001b[39m: \u001b[32mTDigestUDAF\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mTDigestUDAF\u001b[39m(\u001b[32m0.2\u001b[39m, \u001b[32m25\u001b[39m)\n",
       "\u001b[36mres6_2\u001b[39m: \u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(<function1>, DoubleType, \u001b[32mNone\u001b[39m)\n",
       "\u001b[36mres6_3\u001b[39m: \u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(<function1>, DoubleType, \u001b[32mNone\u001b[39m)\n",
       "\u001b[36mres6_4\u001b[39m: \u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(<function1>, DoubleType, \u001b[32mNone\u001b[39m)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"wordcount\", (text: String)=>text.split(\" \").filter(_.length > 0).length)\n",
    "val sketchCDF = tdigestUDAF[Double].delta(0.2).maxDiscrete(25)\n",
    "spark.udf.register(\"p50\", (c:Any)=>c.asInstanceOf[TDigestSQL].tdigest.cdfInverse(0.5))\n",
    "spark.udf.register(\"p90\", (c:Any)=>c.asInstanceOf[TDigestSQL].tdigest.cdfInverse(0.9))\n",
    "spark.udf.register(\"p99\", (c:Any)=>c.asInstanceOf[TDigestSQL].tdigest.cdfInverse(0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell attaches to a kafka stream, unpacks the kafka message data, and returns a streaming data-frame called\n",
    "`records` that has fields:\n",
    "\n",
    "* user_id  (string user id)\n",
    "* text (text of a social media post)\n",
    "* wordcount (the number of words in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [key: binary, value: binary ... 5 more fields]\n",
       "\u001b[36mvalues\u001b[39m: \u001b[32mDataFrame\u001b[39m = [value: string]\n",
       "\u001b[36mstructure\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"text\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"user_id\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"update_id\"\u001b[39m, StringType, true, {})\n",
       ")\n",
       "\u001b[36mrecords\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_id: string, text: string ... 1 more field]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "  .option(\"subscribe\", \"social-firehose\")\n",
    "  .load()\n",
    "val values = df.select(($\"value\").cast(StringType))\n",
    "val structure = StructType(Seq(\"text\",\"user_id\",\"update_id\").map{f=>StructField(f, StringType, true)})\n",
    "val records = values.select(from_json($\"value\", structure).alias(\"json\"))\n",
    "    .select($\"json.user_id\", $\"json.text\")\n",
    "    .select($\"user_id\",\n",
    "            $\"text\",\n",
    "            callUDF(\"wordcount\", $\"text\").alias(\"wordcount\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell demonstrates a streaming query that averages word-count, as grouped by user id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+---+\n",
      "|user_id|avg|\n",
      "+-------+---+\n",
      "+-------+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----------+----+\n",
      "|   user_id| avg|\n",
      "+----------+----+\n",
      "|2064885917|26.0|\n",
      "|5014661643|22.0|\n",
      "|1623497379|21.0|\n",
      "|2743183655|19.0|\n",
      "|8488850176|16.0|\n",
      "|1676389725|16.0|\n",
      "|8348322486|15.0|\n",
      "|2556837131|14.0|\n",
      "|5157754624|10.0|\n",
      "|1955219266| 9.0|\n",
      "|0975976068| 9.0|\n",
      "|3467819502| 4.0|\n",
      "+----------+----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----------+----+\n",
      "|   user_id| avg|\n",
      "+----------+----+\n",
      "|8925539058|31.0|\n",
      "|2064885917|26.0|\n",
      "|4472301601|25.0|\n",
      "|5014661643|22.0|\n",
      "|1623497379|21.0|\n",
      "|2743183655|19.0|\n",
      "|2875279841|18.0|\n",
      "|0153559785|16.0|\n",
      "|8488850176|16.0|\n",
      "|1676389725|16.0|\n",
      "|8348322486|15.0|\n",
      "|3161332818|15.0|\n",
      "|2556837131|14.0|\n",
      "|7358241084|14.0|\n",
      "|7902498897|11.0|\n",
      "|3932624939|10.0|\n",
      "|5157754624|10.0|\n",
      "|1955219266| 9.0|\n",
      "|0975976068| 9.0|\n",
      "|1843039724| 9.0|\n",
      "+----------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+----------+----+\n",
      "|   user_id| avg|\n",
      "+----------+----+\n",
      "|6542842568|37.0|\n",
      "|8925539058|31.0|\n",
      "|1859138320|29.0|\n",
      "|2064885917|26.0|\n",
      "|4472301601|25.0|\n",
      "|6980799924|23.0|\n",
      "|6321186265|23.0|\n",
      "|5014661643|22.0|\n",
      "|6123726103|22.0|\n",
      "|7393344604|22.0|\n",
      "|1623497379|21.0|\n",
      "|2743183655|19.0|\n",
      "|2875279841|18.0|\n",
      "|3312899400|18.0|\n",
      "|4941283327|18.0|\n",
      "|0153559785|16.0|\n",
      "|8488850176|16.0|\n",
      "|1676389725|16.0|\n",
      "|8348322486|15.0|\n",
      "|3161332818|15.0|\n",
      "+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mt\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [user_id: string, avg: double]\n",
       "\u001b[36mquery\u001b[39m: \u001b[32mstreaming\u001b[39m.\u001b[32mStreamingQuery\u001b[39m = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@30d3740c\n",
       "\u001b[36mres8_2\u001b[39m: \u001b[32mBoolean\u001b[39m = false"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t = records.groupBy($\"user_id\")\n",
    "    .agg(avg($\"wordcount\").alias(\"avg\"))\n",
    "    .orderBy($\"avg\".desc)\n",
    "val query = t.writeStream\n",
    "  .trigger(Trigger.ProcessingTime(\"15 seconds\"))\n",
    "  .outputMode(\"complete\")\n",
    "  .format(\"console\")\n",
    "  .start()\n",
    "query.awaitTermination(50 * 1000)\n",
    "Thread.sleep(3 * 1000)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell demonstrates the use of a custom UDAF to aggregate a sketch of the distribution of some streaming numeric\n",
    "data. The aggregation is being grouped by a time window. \n",
    "Here we are sketching the distribution of word counts, and then calling some UDFs to extract percentiles from that sketch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+---+\n",
      "|p50|p90|\n",
      "+---+---+\n",
      "+---+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------+----+\n",
      "|               p50| p90|\n",
      "+------------------+----+\n",
      "|15.333333333333334|33.0|\n",
      "|15.333333333333334|33.0|\n",
      "|15.333333333333334|33.0|\n",
      "+------------------+----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------+------------------+\n",
      "|               p50|               p90|\n",
      "+------------------+------------------+\n",
      "|15.333333333333334|              33.0|\n",
      "|15.333333333333334|              33.0|\n",
      "|              18.0|30.200000000000003|\n",
      "|              18.0|              30.6|\n",
      "|              18.0|30.200000000000003|\n",
      "+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------------------+------------------+\n",
      "|               p50|               p90|\n",
      "+------------------+------------------+\n",
      "|15.333333333333334|              33.0|\n",
      "|              18.0|              29.5|\n",
      "|15.333333333333334|              33.0|\n",
      "|              18.0|30.200000000000003|\n",
      "|              18.0|              29.5|\n",
      "|              18.0|              30.6|\n",
      "|              18.0|              29.8|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mt\u001b[39m: \u001b[32mDataFrame\u001b[39m = [p50: double, p90: double]\n",
       "\u001b[36mquery\u001b[39m: \u001b[32mstreaming\u001b[39m.\u001b[32mStreamingQuery\u001b[39m = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@6498f72e\n",
       "\u001b[36mres9_2\u001b[39m: \u001b[32mBoolean\u001b[39m = false"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t = records.withColumn(\"time\", current_timestamp()).groupBy(window($\"time\", \"30 seconds\", \"10 seconds\"))\n",
    "    .agg(sketchCDF($\"wordcount\").alias(\"CDF\"))\n",
    "    .select(callUDF(\"p50\", $\"CDF\").alias(\"p50\"),\n",
    "            callUDF(\"p90\", $\"CDF\").alias(\"p90\"))\n",
    "val query = t.writeStream\n",
    "  .trigger(Trigger.ProcessingTime(\"20 seconds\"))\n",
    "  .outputMode(\"complete\")\n",
    "  .format(\"console\")\n",
    "  .start()\n",
    "query.awaitTermination(65 * 1000)\n",
    "Thread.sleep(3 * 1000)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell I am defining a utility function to make it more convenient for grouping strongly-typed `Dataset` streams by a time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject\u001b[39m \u001b[36mwindowing\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object windowing {\n",
    "    import java.sql.Timestamp\n",
    "    import java.time.Instant\n",
    "    def windowBy[R](f:R=>Timestamp, width: Int) = {\n",
    "        val w = width.toLong * 1000L\n",
    "        (row: R) => {\n",
    "            val tsCur = f(row)\n",
    "            val msCur = tsCur.getTime()\n",
    "            val msLB = (msCur / w) * w\n",
    "            val instLB = Instant.ofEpochMilli(msLB)\n",
    "            val instUB = Instant.ofEpochMilli(msLB+w)\n",
    "            (Timestamp.from(instLB), Timestamp.from(instUB))\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, a streaming query extracts hash-tags from the raw social media text.\n",
    "Here, the data is treated as a strongly-typed streaming `Dataset`.\n",
    "The data is grouped by time window, and a heavy-hitter aggregator is applied to identify the most\n",
    "frequent hash-tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+---+\n",
      "|_1 |_2 |\n",
      "+---+---+\n",
      "+---+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------------------+---------------------------------------------------------+\n",
      "|_1                 |_2                                                       |\n",
      "+-------------------+---------------------------------------------------------+\n",
      "|2018-09-28 23:52:00|Vector((#_,3), (#two,3), (#Elinor,2), (#first,2), (#3,2))|\n",
      "+-------------------+---------------------------------------------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------------------+------------------------------------------------------------+\n",
      "|_1                 |_2                                                          |\n",
      "+-------------------+------------------------------------------------------------+\n",
      "|2018-09-28 23:52:00|Vector((#_,4), (#two,3), (#first,3), (#6.50,3), (#second,3))|\n",
      "+-------------------+------------------------------------------------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------------------+---------------------------------------------------------------------+\n",
      "|_1                 |_2                                                                   |\n",
      "+-------------------+---------------------------------------------------------------------+\n",
      "|2018-09-28 23:52:00|Vector((#Amazon,6), (#_,4), (#6.50,4), (#second,4), (#Hollandaise,4))|\n",
      "+-------------------+---------------------------------------------------------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------------------+-----------------------------------------------------------------------------------------+\n",
      "|_1                 |_2                                                                                       |\n",
      "+-------------------+-----------------------------------------------------------------------------------------+\n",
      "|2018-09-28 23:53:00|Vector((#Elizabeth,4), (#LicketyStik,3), (#theseBIGFRANKS,3), (#Diedrich,2), (#Castle,2))|\n",
      "|2018-09-28 23:52:00|Vector((#Amazon,6), (#_,4), (#6.50,4), (#second,4), (#Hollandaise,4))                    |\n",
      "+-------------------+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-------------------+------------------------------------------------------------------------------------------+\n",
      "|_1                 |_2                                                                                        |\n",
      "+-------------------+------------------------------------------------------------------------------------------+\n",
      "|2018-09-28 23:53:00|Vector((#Elizabeth,5), (#LicketyStik,3), (#theseBIGFRANKS,3), (#Castle,3), (#Yorkshire,3))|\n",
      "|2018-09-28 23:52:00|Vector((#Amazon,6), (#_,4), (#6.50,4), (#second,4), (#Hollandaise,4))                     |\n",
      "+-------------------+------------------------------------------------------------------------------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-------------------+------------------------------------------------------------------------------------------+\n",
      "|_1                 |_2                                                                                        |\n",
      "+-------------------+------------------------------------------------------------------------------------------+\n",
      "|2018-09-28 23:53:00|Vector((#Elizabeth,7), (#LicketyStik,4), (#theseBIGFRANKS,4), (#Castle,4), (#Yorkshire,4))|\n",
      "|2018-09-28 23:52:00|Vector((#Amazon,6), (#_,4), (#6.50,4), (#second,4), (#Hollandaise,4))                     |\n",
      "+-------------------+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mwindowBy60\u001b[39m: (\u001b[32mjava\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mTimestamp\u001b[39m, \u001b[32mString\u001b[39m) => (\u001b[32mjava\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mTimestamp\u001b[39m, \u001b[32mjava\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mTimestamp\u001b[39m) = <function1>\n",
       "\u001b[36mtka\u001b[39m: \u001b[32mTypedColumn\u001b[39m[(\u001b[32mjava\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mTimestamp\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)]] = topkaggregator()\n",
       "\u001b[36mt\u001b[39m: \u001b[32mDataset\u001b[39m[(\u001b[32mjava\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mTimestamp\u001b[39m, \u001b[32mString\u001b[39m)] = [_1: timestamp, _2: string]\n",
       "\u001b[36mquery\u001b[39m: \u001b[32mstreaming\u001b[39m.\u001b[32mStreamingQuery\u001b[39m = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@6bcf6952\n",
       "\u001b[36mres11_4\u001b[39m: \u001b[32mBoolean\u001b[39m = false"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val windowBy60 = windowing.windowBy[(java.sql.Timestamp, String)](_._1, 60)\n",
    "val tka = new org.isarnproject.sketches.udaf.TopKAggregator[(java.sql.Timestamp, String)](_._2).toColumn\n",
    "val t = records\n",
    "  .select(current_timestamp().alias(\"time\"),\n",
    "          explode(split($\"text\", \" \")).alias(\"word\"))\n",
    "  .as[(java.sql.Timestamp, String)]\n",
    "  .filter(_._2(0)=='#')\n",
    "  .groupByKey(windowBy60).agg(tka)\n",
    "  .map { case (tw, tk) => (tw._2, tk.toVector.toString)}\n",
    "val query = t.writeStream\n",
    "  .trigger(Trigger.ProcessingTime(\"20 seconds\"))\n",
    "  .option(\"truncate\", false)\n",
    "  .outputMode(\"complete\")\n",
    "  .format(\"console\")\n",
    "  .start()\n",
    "query.awaitTermination(125 * 1000)\n",
    "Thread.sleep(3 * 1000)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
